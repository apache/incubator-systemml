#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

source("scripts/staging/fm-regression.dml") as fmRegression

source("nn/layers/affine.dml") as affine
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/dropout.dml") as dropout
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax
source("nn/optim/adam.dml") as adam

# function to train

# function to predict

# build_features(matrix A)

# extract_bag_of_characters_features(raw_sample, n_values), ignore_index=True)

# extract_word_embeddings_features(raw_sample), ignore_index=True)

# infer_paragraph_embeddings_features(raw_sample, vec_dim), ignore_index=True)

# extract_bag_of_words_features(raw_sample), ignore_index=True)

# Frame[] load_file (Input: csv)
# Rows and columns index from 1. (-.-)
# no variables are possible to define the filepath of the read() function
# every *.csv file needs a *.csv.mtd file with following content:
#{
#  "data_type": "frame",
#  "value_type": "string",
#  "format": "csv",
#  "header": true,
#  "sep": ",",
#  "author": "local",
#  "created": "2020-12-07 00:00:01 UTC"
#}
load_raw_train_values = function() return (frame[string] data) {
  tableSchema = "string,list(string)";
  data = read("../sherlock-project/data/data/short/100_X_test.csv",
              fill=TRUE,
              header=TRUE
              #schema=tableSchema
              )
  print(toString(data))
  rows = nrow(data)
  cols = ncol(data)
  print("loaded train_values.csv with " + rows + " rows and " + cols + " columns")

  #s = map(data, "d -> UtilFunctions.magicFrame(d)")
  #print(toString(s))


  s = map(data[2,2], "d -> UtilFunctions.getSplittedStringAsList(d)")
  print(toString(s))
  l_s = list(as.scalar(s))
  print(toString(length(l_s)))
/*

  newFrame=matrix(0,rows=rows, cols=2)
  newFrame=as.frame(newFrame)
  newFrame = cbind(newFrame, data)
  print(toString(newFrame))
  #extra_col_frame=frame(0,rows=rows, cols=1)
  for(i in 1:rows, check=0) {
    newFrame[i,1] = map(data[i,2], "d -> d.split(\"\'[ ]*,[ ]*\'\").length")
    s = (data[i,2])
    s_l = list(map(s, "s -> s.split(\"\'[ ]*,[ ]*\'\")"))
    print(toString(s))
    print(toString(s_l))
  }
  #print(toString(data))
  #print(toString(newFrame))
*/
 # print("split: " + toString(list(data[1,2])))
  S = "ich, habe, hunber"
  #res = S/","
  #print(res)
  l = list("ich")
  #splitted = strsplit(S,",")
  #list = list(strsplit(S, ",")[[1]])
  #append("du", l)
  #print(toString(l))


  #print(S.split(",")[1])
}

load_processed_test_values = function() return (frame[string] data) {

  tableSchema = "string,string";
  data = read("../sherlock-project/data/data/short/100_X_test.csv",
  fill=TRUE,
  header=TRUE
  #schema=tableSchema
  )
  rows = nrow(data)
  cols = ncol(data)
  print("loaded X_test_short.csv with " + rows + " rows and " + cols + " columns")
}

load_processed_train_values = function() return (frame[string] data) {

  tableSchema = "string,string";
  data = read("../sherlock-project/data/data/short/100_X_train.csv",
  fill=TRUE,
  header=TRUE
  #schema=tableSchema
  )
  rows = nrow(data)
  cols = ncol(data)
  print("loaded X_train.csv with " + rows + " rows and " + cols + " columns")
}

load_processed_val_values = function() return (frame[string] data) {

  tableSchema = "string,string";
  data = read("../sherlock-project/data/data/short/100_X_val.csv",
  fill=TRUE,
  header=TRUE
  #schema=tableSchema
  )
  rows = nrow(data)
  cols = ncol(data)
  print("loaded X_val.csv with " + rows + " rows and " + cols + " columns")
}

load_processed_test_labels = function() return (frame[string] data) {
  tableSchema = "string,string";
  data = read("../sherlock-project/data/data/short/100_y_test.csv",
  fill=TRUE,
  header=TRUE
  #schema=tableSchema
  )
  rows = nrow(data)
  cols = ncol(data)
  print("loaded y_test_short.csv with " + rows + " rows and " + cols + " columns")
}

load_processed_train_labels = function() return (frame[string] data) {
  tableSchema = "string,string";
  data = read("../sherlock-project/data/data/short/100_y_train.csv",
  fill=TRUE,
  header=TRUE
  #schema=tableSchema
  )
  rows = nrow(data)
  cols = ncol(data)
  print("loaded y_train.csv with " + rows + " rows and " + cols + " columns")
}

load_processed_val_labels = function() return (frame[string] data) {
  tableSchema = "string,string";
  data = read("../sherlock-project/data/data/short/100_y_val.csv",
  fill=TRUE,
  header=TRUE
  #schema=tableSchema
  )
  rows = nrow(data)
  cols = ncol(data)
  print("loaded y_val.csv with " + rows + " rows and " + cols + " columns")
}

transform_values = function(frame[string] data) return (matrix[double] m_data) {
  #remove index row
  rows = nrow(data)
  cols = ncol(data)
  data = data[1:rows,2:cols]
  rows = nrow(data)
  cols = ncol(data)
  print("loaded *values.csv with " + rows + " rows and " + cols + " columns")

  #replace True/Fasle with 1/0
  data = map(data, "d -> d.replace(\"True\",\"1\")")
  data = map(data, "d -> d.replace(\"False\",\"0\")")
  m_data = as.matrix(data)
}

transform_encode_labels = function(frame[string] data) return (matrix[double] m_data , frame[string] meta_data) {
  #remove index row
  rows = nrow(data)
  cols = ncol(data)
  data = data[1:rows,2:cols]
  rows = nrow(data)
  cols = ncol(data)
  print("loaded y_test_short.csv with " + rows + " rows and " + cols + " columns")
  print(toString(data[2]))

  #replace label with number
  transformSpec = read("../sherlock-project/data/data/short/transform_y_labels.json", data_type="scalar", value_type="string");
  [m_data, meta_data] = transformencode(target=data,
  spec=transformSpec)
  print(toString(m_data[2]))
  write(meta_data, "../sherlock-project/data/data/short/transform_y_labels")
}

transform_apply_labels = function(frame[string] data, frame[string] meta_data) return (matrix[double] m_data) {
  #remove index row
  rows = nrow(data)
  cols = ncol(data)
  data = data[1:rows,2:cols]
  rows = nrow(data)
  cols = ncol(data)
  print("loaded y_test_short.csv with " + rows + " rows and " + cols + " columns")
  print(toString(data[2]))

  #replace label with number
  transformSpec = read("../sherlock-project/data/data/short/transform_y_labels.json", data_type="scalar", value_type="string");
  transformMeta = read("../sherlock-project/data/data/short/transform_y_labels");
  m_data = transformapply(target=data,
                  spec=transformSpec,
                  meta=meta_data)
  #print(toString(m_data[2]))
}

train = function(matrix[double] X_train, matrix[double] y_train, matrix[double] X_val, matrix[double] y_val, int hidden_layer_neurons)
        return (matrix[double] W1, matrix[double] b1, matrix[double] W2, matrix[double] b2, matrix[double] W3, matrix[double] b3)  {

  # Generate input data
  N = nrow(X_train) # num examples
  D = ncol(X_train) # num features
  t = 1 # num targets
  
  # Create network:
  # batch -> affine1 -> relu1 -> dropout1 -> affine2 -> relu2 -> affine3 -> softmax
  H1 = hidden_layer_neurons # number of neurons in 1st hidden layer
  H2 = hidden_layer_neurons # number of neurons in 2nd hidden layer
  p = 0.3  # dropout probability
  [W1, b1] = affine::init(D, H1, -1)
  [W2, b2] = affine::init(H1, H2, -1)
  [W3, b3] = affine::init(H2, t, -1)

  # Initialize Adams parameter 
  lr = 0.0001  # learning rate
  mu = 0.5  # momentum
  decay = 0.0001  # learning rate decay constant for weight decay

  beta1   = 0.9;       # [0, 1)
  beta2   = 0.999;     # [0, 1)
  epsilon = 0.00000001;
  #t       = 0; # t von oben?

  # Adams optimizer
  [mW1, vW1] = adam::init(W1); [mb1, vb1] = adam::init(b1)
  [mW2, vW2] = adam::init(W2); [mb2, vb2] = adam::init(b2)
  [mW3, vW3] = adam::init(W3); [mb3, vb3] = adam::init(b3)

  # Optimize
  print("Starting optimization")
  batch_size = 256 #?
  epochs = 100
  iters = floor(N / batch_size)
  for (e in 1:epochs) {
    for(i in 1:iters) {
      # Get next batch
      X_batch = X_train[i:i+batch_size-1,]
      y_batch = y_train[i:i+batch_size-1,]

      # Compute forward pass
      ## layer 1:
      out1 = affine::forward(X_batch, W1, b1)
      outr1 = relu::forward(out1)
      [outd1, maskd1] = dropout::forward(outr1, p, -1)
      ## layer 2:
      out2 = affine::forward(outd1, W2, b2)
      outr2 = relu::forward(out2)
      [outd2, maskd2] = dropout::forward(outr2, p, -1)
      ## layer 3:
      out3 = affine::forward(outd2, W3, b3)
      probs = softmax::forward(out3)

      # Compute loss
      loss = cross_entropy_loss::forward(probs, y_batch)
      print("Cross entropy loss: " + loss)

      # Compute backward pass
      ## loss:
      dprobs = cross_entropy_loss::backward(probs, y_batch)
      ## layer 3:
      dout3 = softmax::backward(dprobs, out3)
      [doutd2, dW3, db3] = affine::backward(dout3, outd2, W3, b3)
      ## layer 2:
      doutr2 = dropout::backward(doutd2, outr2, p, maskd2)
      dout2 = relu::backward(doutr2, out2)
      [doutd1, dW2, db2] = affine::backward(dout2, outd1, W2, b2)
      ## layer 1:
      doutr1 = dropout::backward(doutd1, outr1, p, maskd1)
      dout1 = relu::backward(doutr1, out1)
      [dX_batch, dW1, db1] = affine::backward(dout1, X_batch, W1, b1)

      # Optimize with Adam
      [W1, mW1, vW1] = adam::update(W1, dW1, lr, beta1, beta2, epsilon, t, mW1, vW1)
      [b1, mb1, vb1] = adam::update(b1, db1, lr, beta1, beta2, epsilon, t, mb1, vb1)
      [W2, mW2, vW2] = adam::update(W2, dW2, lr, beta1, beta2, epsilon, t, mW2, vW2)
      [b2, mb2, vb2] = adam::update(b2, db2, lr, beta1, beta2, epsilon, t, mb2, vb2)
      [W3, mW3, vW3] = adam::update(W3, dW3, lr, beta1, beta2, epsilon, t, mW3, vW3)
      [b3, mb3, vb3] = adam::update(b3, db3, lr, beta1, beta2, epsilon, t, mb3, vb3)
    }

    # Anneal momentum towards 0.999
    mu = mu + (0.999 - mu)/(1+epochs-e)
    # Decay learning rate
    lr = lr * decay
  }
}

m_sherlock = function (matrix[double] A) return (double m) {
  m = sum(A)/nrow(A)
  processed_test_values = transform_values(load_processed_test_values())
  processed_train_values = transform_values(load_processed_train_values())
  processed_val_values = transform_values(load_processed_val_values())

  [processed_test_labels, meta_data] = transform_encode_labels(load_processed_test_labels())
  processed_train_labels = transform_apply_labels(load_processed_train_labels(), meta_data)
  processed_val_labels = transform_apply_labels(load_processed_val_labels(), meta_data)
  #[w0, W, V] = fmRegression::train(processed_train_values, processed_train_labels,
    #processed_val_values, processed_val_labels)
  #write(w0, "../sherlock-project/data/data/short/w0")
  #write(W, "../sherlock-project/data/data/short/W")
  #write(V, "../sherlock-project/data/data/short/V")

  #train

  #character level
  hidden_layer_neurons = 300
  [W1, b1, W2, b2, W3, b3] = train(processed_train_values, processed_train_labels,
    processed_val_values, processed_val_labels, hidden_layer_neurons)

  write(W1, "../sherlock-project/data/data/short/W1")
  write(b1, "../sherlock-project/data/data/short/b1")
  write(W2, "../sherlock-project/data/data/short/W2")
  write(b2, "../sherlock-project/data/data/short/b2")
  write(W3, "../sherlock-project/data/data/short/W3")
  write(b3, "../sherlock-project/data/data/short/b3")
}







