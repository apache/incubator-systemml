
# ---------------------------------------------------------------------------------------------
# OUTPUT:
# Matrix M where each column corresponds to a node in the learned tree and each row contains the following information:
#  M[1,j]: id of node j (in a complete binary tree)
#  M[2,j]: Offset (no. of columns) to left child of j if j is an internal node, otherwise 0
#  M[3,j]: Feature index of the feature (scale feature id if the feature is scale or categorical feature id if the feature is categorical)
#    that node j looks at if j is an internal node, otherwise 0
#  M[4,j]: Type of the feature that node j looks at if j is an internal node: holds the same information as R input vector
#  M[5,j]: If j is an internal node: 1 if the feature chosen for j is scale, otherwise the size of the subset of values
#    stored in rows 6,7,... if j is categorical
#    If j is a leaf node: 0
#  M[6:,j]: If j is an internal node: Threshold the example's feature value is compared to is stored at M[6,j] if the feature chosen for j is scale,
#     otherwise if the feature chosen for j is categorical rows 6,7,... depict the value subset chosen for j
#     If j is a leaf node 1 if j is impure and the number of samples at j > threshold, otherwise 0
# -------------------------------------------------------------------------------------------

m_xgboost = function(Matrix[Double] X, Matrix[Double] y) return (Matrix[Double] M) {
  print("[DEBUG] inside xgboost")

  # test if input correct
  assert(nrow(X) == nrow(y))
  assert(ncol(y) == 1)

  M = matrix(0,rows=6,cols=0)
  init_prediction = median(y)

  node_queue = matrix(1, rows=1, cols=1)     # Add first Node
  curr_rows_queue = matrix(1, rows=nrow(X), cols=1)
  gain_queue = matrix(1, rows=1, cols=1)
  node_queue_len = 1
  depth = 6
  level = 0

  # adjust this here if we create more trees to create new prediction values
  while (node_queue_len > 0) {
    has_child = FALSE
    [node_queue, node] = dataQueuePop(node_queue)
    [curr_rows_queue, curr_rows_vector] = dataQueuePop(curr_rows_queue)
    level = log(as.scalar(node), 2) + 1

    available_rows = calcAvailableRows(curr_rows_vector)

    [curr_X, loop_y, curr_X_full] = updateMatrices(X, y, curr_rows_vector)

    best_feature_index = 0.00
    if(available_rows > 1 & depth > level) {
      best_feature_index = findBestFeature(X=curr_X, y=loop_y)

      # maybe for forest change here prediction?
      residual_matrix = calculateResiduals(loop_y, median(loop_y))
      printMatrix(residual_matrix, "the cgialculated residuals")

      similarity_score = calculateSimilarityScore(residual_matrix)

      [best_split_threshold, best_gain] = findBestSplit(curr_X[,best_feature_index], similarity_score)
      has_child = best_gain > 0
    }

    if (has_child) {
      [left, right] = calculateChildNodes(node)
      node_queue = dataQueuePush(left, right, node_queue)

      [left_row_vec, right_row_vec] = splitMatrixByValueLoop(curr_X_full[,best_feature_index], X[,best_feature_index], best_split_threshold)

      curr_rows_queue = dataQueuePush(left_row_vec, right_row_vec, curr_rows_queue)

      offset = ncol(node_queue) - 1
      M = addOutputRow(M, node, offset, best_feature_index, best_split_threshold)
    }
    else {
      M = addOutputRow(M, node, 0.0, best_feature_index, 0.0)
    }
    node_queue_len = ncol(node_queue)
    printMatrix(M, "FINAL MATRIX")
  }

  # todo pruning
  # todo create next tree depending on previous
  print("[DEBUG] Return xgboost")
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    node: a 1xn matrix (node containing values)
# OUTPUT:   left: the new left node id
# OUTPUT:   right: the new right node id
calculateChildNodes = function(Matrix[Double] node)  return(Matrix[Double] left, Matrix[Double] right) {
  left = node * 2.0
  right = node * 2.0 + 1.0
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    M: the nxn output matrix
# INPUT:    node: the current node
# INPUT:    offset: offset to the left child
# INPUT:    used_col: which feature got used
# INPUT:
# INPUT:    threshold: the value at the node we seperated it
# OUTPUT:   new_M: nxn output Matrix
addOutputRow = function(
  Matrix[Double] M,
  Matrix[Double] node,
  Double offset,
  Double used_col,
  Double threshold
) return (Matrix[Double] new_M) {
  current_node = matrix(0, rows=6, cols=1)
  current_node[1,1] = node[1,1] # node id
  current_node[2,1] = offset # offset to left child
  current_node[3,1] = used_col # features used
  # change here to different stuff if it is classification
  current_node[4,1] = 1 # continuous data
  # also change here if it is classification
  if (threshold < 1.00) {
    current_node[5,1] = 0 # number of values in threshhold // currently 0 if leaf node
  } else {
    current_node[5,1] = 1
  }
  current_node[6,1] = threshold
  if (ncol(M) == 0 & nrow(M) == 0) {
    new_M = current_node
  } else {
    new_M = cbind(M, current_node)
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    X: a nxn matrix (X Matrix from input)
# INPUT:    y: a 1xn matrix (y Matrix from input)
# INPUT:    vector: a 1xn matrix (contains 1 for relevant and 0 for non-relevant value)
# OUTPUT:   new_X: the new X containing only the relevant rows
# OUTPUT:   new_y: the new y containing only the relevant rows
# OUTPUT:   new_X_full: the new X containing the relevant rows + 0 for non_relevant
updateMatrices = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] vector)
  return(Matrix[Double] new_X, Matrix[Double] new_y, Matrix[Double] new_X_full) {
  # change here to have the full matrices but not to cut it from 4x4 to 2x2 rather 4x4 but with 0 in the unused fields and then we gucci
  new_X = matrix(0, rows = 0, cols = 0)
  new_y = matrix(0, rows = 0, cols = 0)
  zero_vec = matrix(0, rows=nrow(vector), cols=ncol(X))
  new_X_full = matrix(0, rows = 0, cols=0)
  for(i in 1:nrow(vector)) {
    if(as.scalar(vector[i,]) > 0.0) {
      if (ncol(new_X) == 0 & ncol(new_X_full) == 0) {
        new_X = X[i,]
        new_y = y[i,]
        new_X_full = X[i,]
      } else if (ncol(new_X) == 0 & ncol(new_X_full) != 0) {
        new_X = X[i,]
        new_y = y[i,]
        new_X_full = rbind(new_X_full, X[i,])
      } else {
        new_X = rbind(new_X, X[i,])
        new_y = rbind(new_y, y[i,])
        new_X_full = rbind(new_X_full, X[i,])
      }
    } else {
      if (ncol(new_X_full) == 0) {
       new_X_full = zero_vec[i,]
      } else {
       new_X_full = rbind(new_X_full, zero_vec[i,])
      }
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    vector: a 1xn matrix (current datapoints which are interesting)
# OUTPUT:   available_elements: nr. of available datapoints
calcAvailableRows = function(Matrix[Double] vector) return(Double available_elements){
  len = nrow(vector)
  available_elements = 0.0
  for (index in 1:len) {
    element = as.scalar(vector[index,])
    if(element > 0.0) {
      available_elements = available_elements + 1.0
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    queue: a nxn matrix (queue containing values)
# OUTPUT:   new_queue: the new queue after the first vector gets popped
# OUTPUT:   node_vec: the popped vector (1xn) from the queue
dataQueuePop = function(Matrix[Double] queue)  return (Matrix[Double] new_queue, Matrix[Double] node_vec) {
  node_vec = matrix(queue[,1], rows=1, cols=nrow(queue)) # reshape to force the creation of a new object
  node_vec = matrix(node_vec, rows=nrow(queue), cols=1)  # reshape to force the creation of a new object
  len = ncol(queue)
  if (len < 2) {
    new_queue = matrix(0,0,0)
  } else {
    new_queue = matrix(queue[,2:ncol(queue)], rows=nrow(queue), cols=ncol(queue)-1)
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    left: a 1xn matrix
# INPUT:    right: a 1xn matrix
# INPUT:    queue: a nxn matrix (queue containing values)
# OUTPUT:   new_queue: the new queue nxn matrix
dataQueuePush = function(Matrix[Double] left, Matrix[Double] right, Matrix[Double] queue)  return (Matrix[Double] new_queue) {
  len = ncol(queue)
  if(len <= 0) {
    new_queue = cbind(left, right)
  } else {
    new_queue = cbind(queue, left, right)
  }
}


findBestFeature = function(Matrix[Double] X, Matrix[Double] y, boolean verbose = FALSE) return (Integer lowest_residuals_index) {

   printDebug("Finding Best feature")

   lowest_residuals = 0
   lowest_residuals_index = 1

   for(i in 1: ncol(X)) {
        current_feature = X[,i]
        weights = lm(X=current_feature, y=y, verbose=verbose)
        y_residual = y - current_feature %*% weights
        avg_res = abs(sum (y_residual) / nrow (X));

        if(verbose) {
            printDebug("currentFeature col " + toString(i))
            printDebug("weights of lm " + toString(weights))
            printDebug("feature residuals: " + avg_res)
            print("--------------------------------------")
        }

        if(i == 1 | avg_res < lowest_residuals) {
            lowest_residuals = avg_res
            lowest_residuals_index = i
        }
   }
   printDebug("best feature for root note is: " + toString(lowest_residuals_index) + " with residuals: " + toString(lowest_residuals))

}


#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    one_featureX: a 1xn matrix (one feature with all values)
# OUTPUT:   best_split: the best split (highest gain indicates best splitting of datasets)
findBestSplit = function(Matrix[Double] one_featureX, Double sim_score_parent)
  return (Double best_split, Double best_gain)
{
    assert(ncol(one_featureX) == 1)
    best_split = 0
    best_gain = 0
    best_sim_score_left = 0
    best_sim_score_right = 0
    printDebug("inside find best split")

    # order entries by feature value
    ordered_X = orderFeature(one_featureX)
    printMatrix(ordered_X, "ordered X")

    # iterate over all averages between 2 points
    for(i in 1: (nrow(ordered_X)-1)) {

        current_split = average(ordered_X[i,], ordered_X[i+1,])
        printDebug("first average = " + toString(current_split))
        [left, right] = splitMatrixByValue(one_featureX, current_split)
        printMatrix(left, "left matrix")
        printMatrix(right, "right matrix")

        # calculate similarity score for that split and calculate gain
        sim_score_left = calculateSimilarityScore(left)
        sim_score_right = calculateSimilarityScore(right)
        printDebug("sim score left " + toString(sim_score_left) + "   sim score right " + toString(sim_score_right))
        current_gain = sim_score_left + sim_score_right - sim_score_parent
        printDebug("current gain " + toString(current_gain))

        if(current_gain > best_gain)
        {
            best_gain = current_gain
            best_split = current_split
            best_sim_score_left = sim_score_left
            best_sim_score_right = sim_score_right
            printDebug("best gain changed to  " + toString(best_gain) + " at value " + toString(best_split))
        }
    }
}




#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    X: a 1xn matrix (one feature with all values)
#           value: the border to separate the values
#           loop: boolean to check if used in loop and returns then other values
# OUTPUT:   left: a 1xn matrix with all values smaller than "value"
#           right: a 1xn matrix with all values larger than "value"
splitMatrixByValue = function(Matrix[Double] X, Double value) return (Matrix[Double] left, Matrix[Double] right) {
  assert(ncol(X) == 1)
  printDebug("in splitMatrixByValue")
  printDebug("split at " + value)
  left = matrix(0, rows=0, cols=1)
  right = matrix(0, rows=0, cols=1)

  for(i in 1:nrow(X)) {
    printDebug("current to look " + as.scalar(X[i,]))
    if(as.scalar(X[i,]) < value) {
      left = rbind(left,X[i,])
    }
    else {
      right = rbind(right, X[i,])
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    curr_X: a 1xn matrix (one feature with current values)
#           X:     a 1xn matrix the feature of the X-Matrix with all values
#           value: the border to separate the values
# OUTPUT:   left: a 1xn matrix with 1.0 for if the value is smaller or 0.0 if it is bigger
#           right: a 1xn matrix with 1.0 for if the value is bigger or 0.0 if it is smaller
splitMatrixByValueLoop = function(Matrix[Double] curr_X, Matrix[Double] X, Double value) return (Matrix[Double] left, Matrix[Double] right) {
  left = matrix(0, rows=0, cols=1)
  right = matrix(0, rows=0, cols=1)
  one_vec = matrix(1, rows=1, cols=1)
  zero_vec = matrix(0, rows=1, cols=1)

  for(i in 1:nrow(X)) {
    if (as.scalar(X[i,]) == as.scalar(curr_X[i,])) {
      if (as.scalar(X[i,]) < value) {
        left = rbind(left, one_vec[1,])
        right = rbind(right, zero_vec[1,])
      }
      else {
        right = rbind(right, one_vec[1,])
        left = rbind(left, zero_vec[1,])
      }
    } else {
      left = rbind(left, zero_vec[1,])
      right = rbind(right, zero_vec[1,])
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    row_vector: a 1xn matrix (one feature with all residuals)
# OUTPUT:   similarity_score: the similarity score of the residuals
calculateSimilarityScore = function (matrix[Double] row_vector) return (Double similarity_score) {
  lambda = 0.0; # TODO
  similarity_score = (sum(row_vector)^2) / (nrow(row_vector) + lambda);
  printDebug("Calculated similarity score: " + similarity_score);
}


#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    row_vector: a 1xn matrix (one feature with all rows)
#           prediction: the current prediction value
# OUTPUT:   the residuals from the "prediction" as 1xn matrix
calculateResiduals = function (matrix[Double] row_vector, Double prediction) return (matrix[Double] residuals_row) {
   assert(ncol(row_vector) == 1)
   residuals_row = row_vector - prediction;
}

orderFeature = function (matrix[double] row_vector) return (matrix[double] row_vector_ordered) {
    row_vector_ordered = order(target=row_vector, by=1, decreasing=FALSE, index.return=FALSE);
}

average = function(Matrix[Double] x, Matrix[Double] y) return (Double avg) {
    assert(ncol(x) == 1 & nrow(x) == 1)
    assert(ncol(y) == 1 & nrow(y) == 1)
    avg = (as.scalar(x) + as.scalar(y)) / 2
}

printDebug = function(string variable) return() {
    print("[DEBUG] " + variable)
}

printMatrix = function(Matrix[Double] m, string name) return() {
    print("--------------------------------------")
    print(":: [MATRIX] " + name + "  (col:" + ncol(m) + ",row:" + nrow(m) + ")")
    for(i in 1:nrow(m)) {
        print(":(" + i + ")   " + toString(m[i,], linesep=" ", sep="    "))
    }
    print("--------------------------------------")
}