#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT IMPLEMENTS CLASSIFICATION TREES WITH BOTH SCALE AND CATEGORICAL FEATURES
#
# INPUT         PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE     DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             String   ---          Location to read feature matrix X; note that X needs to be both recoded and dummy coded
# Y 			String   ---		  Location to read label matrix Y; note that Y needs to be both recoded and dummy coded
# R   	  		String   " "	      Location to read the matrix R which for each feature in X contains the following information
#										- R[,1]: column ids
#										- R[,2]: start indices
#										- R[,3]: end indices
#									  If R is not provided by default all variables are assumed to be scale
# bins          Int 	 20			  Number of equiheight bins per scale feature to choose thresholds
# depth         Int 	 25			  Maximum depth of the learned tree
# num_leaf      Int      10           Number of samples when splitting stops and a leaf node is added
# num_samples   Int 	 3000		  Number of samples at which point we switch to in-memory subtree building
# impurity      String   "Gini"    	  Impurity measure: entropy or Gini (the default)
# M             String 	 ---	   	  Location to write matrix M containing the learned tree
# O     		String   " "          Location to write the training accuracy; by default is standard output
# S_map			String   " "		  Location to write the mappings from scale feature ids to global feature ids
# C_map			String   " "		  Location to write the mappings from categorical feature ids to global feature ids
# fmt     	    String   "text"       The output format of the model (matrix M), such as "text" or "csv"
# ---------------------------------------------------------------------------------------------
# OUTPUT:
# Matrix M where each column corresponds to a node in the learned tree and each row contains the following information:
#	 M[1,j]: id of node j (in a complete binary tree)
#	 M[2,j]: Offset (no. of columns) to left child of j if j is an internal node, otherwise 0
#	 M[3,j]: Feature index of the feature (scale feature id if the feature is scale or categorical feature id if the feature is categorical)
#			 that node j looks at if j is an internal node, otherwise 0
#	 M[4,j]: Type of the feature that node j looks at if j is an internal node: 1 for scale and 2 for categorical features,
#		     otherwise the label that leaf node j is supposed to predict
#	 M[5,j]: If j is an internal node: 1 if the feature chosen for j is scale, otherwise the size of the subset of values
#			 stored in rows 6,7,... if j is categorical
#			 If j is a leaf node: number of misclassified samples reaching at node j
#	 M[6:,j]: If j is an internal node: Threshold the example's feature value is compared to is stored at M[6,j] if the feature chosen for j is scale,
#			  otherwise if the feature chosen for j is categorical rows 6,7,... depict the value subset chosen for j
#	          If j is a leaf node 1 if j is impure and the number of samples at j > threshold, otherwise 0
# -------------------------------------------------------------------------------------------
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemDS.jar -f decision-tree.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y R=INPUT_DIR/R M=OUTPUT_DIR/model
#     				                 				   bins=20 depth=25 num_leaf=10 num_samples=3000 impurity=Gini fmt=csv



# ----------------------------------------------------------------------------------------------------------------------
# Pseudo Code:
# All ignoring NULL Features/COLUMNS/ROWS
# calcImpurity(frame = Frame[], col, labels = Array[])
#       returns impurity: Scale and splittingCriteria: Scalar or List{FeatureClassIndices}
# calcBestSplittingCriteria(frame = Frame[], labels = Array[])
#       runs through all features in frame and calculates the impurity
#       returns column with the best (lowest) Impurity, and the splittingCriteria
# splitData(frame = Frame, splittingCriteria: SplittingCriteria)
#       returns FalseFrame and TrueFrame according to the Splitting Criteria (to keep the indices true fill unwanted Data with NULL)
# calcLeftNode(i: Int) = i * 2
#       returns it left NodeInBinTree (for Example: calcLeftNode(1) = 2, calcLeftNode(2) = 4, calcLeftNode(3) = 6)
# -------------------------
# inputData = read(X)
# labels = read(Y)
# inputLabels = (R == " ")? USE_SCALAR : read(R) USE_IT_TO_DETERMINE_IF_FEATURE_IS_SCALE_OR_LABELED
#
# featureQueue = [inputData]
# nodeQueue = [1]
# misclassifiedQueue = [-1]
# impurityQueue = [1.0]
# depthQueue = [0]
# decisionTreeOutputMatrix = [[]]
#
# while (featureQueue.isNotEmpty) {
#
#   currentDataFrame = featureQueue.getAndRemoveFirst()             --------| // Col and Row indices
#   currentNode = nodeQueue.getAndRemoveFirst()                             |
#   currentMisclassified = misclassifiedQueue.getAndRemoveFirst()           | - Could be one Class, if this was an Object Oriented Language
#   parentImpurity = impurityQueue.getAndRemoveFirst()                      |
#   parentDepth = depthQueue.getAndRemoveFirst()                    --------|
#
#   impurity, splittingCriteria = calcBestSplittingCriteria(currentDataFrame, labels)
#   if (impurity < parentImpurity AND parentDepth + 1 <= depth) {
#     #------------------------------------IS NODE!------------------------------------#
#     numMisclassifiedFalse, falseFrame, numMisclassifiedTrue, trueFrame = splitData(currentDataFrame, splittingCriteria)
#     featureQueue.putAll(falseFrame, trueFrame)
#     misclassifiedQueue.putAll(numMisclassifiedFalse, numMisclassifiedTrue)
#     nodeQueue.putAll(calcLeftNode(currentNode), calcLeftNode(currentNode) + 1)
#     impurityQueue.putAll(impurity, impurity)
#     depthQueue.putAll(parentDepth + 1, parentDepth + 1)
#
#     decisionTreeOutputMatrix.addColumn( [id = currentNode,
#                                          offset = length(featureQueue) - 1,
#                                          featureIndex = splittingCriteria.Column,
#                                          featureType = splittingCriteria.Type,                # 1 for scale and 2 for categorical features
#                                          numCategories = length(splittingCriteria.threshold), # look at description on the top of the page
#                                          splittingCriteria.threshold                          # threshold for scale else categories (element of operation)
#                                          ## append Categories if necessary ])
#   } else {
#     #------------------------------------IS LEAF!------------------------------------#
#     decisionTreeOutputMatrix.addColumn( [id = currentNode,
#                                          offset = 0,
#                                          featureIndex = 0
#                                          featureType = currentNode mod 2,                # (0 for false Branch, 1 for True Branch)
#                                          numCategories = currentMisclassified,
#                                          $1 if currentNode is impure and the number of samples at currentDataFrame > threshold, otherwise 0$])
#   }
# }

# --------------------- Missuses matrix as a queue for vectors ---------------------------------------------------------
dataQueueLength = function(Matrix[Double] queue)  return (Double len) {
    len = ncol(queue)
}

dataQueuePop = function(Matrix[Double] queue)  return (Matrix[Double] new_queue, Matrix[Double] node) {
    node = matrix(queue[,1], rows=1, cols=nrow(queue))     # reshape to force the creation of a new object
    node = matrix(node, rows=nrow(queue), cols=1)          # reshape to force the creation of a new object
    len = dataQueueLength(queue)
    if (len < 2) {
        new_queue = matrix(0,0,0)
    } else {
        new_queue = matrix(queue[,2:ncol(queue)], rows=nrow(queue), cols=ncol(queue)-1)
    }
}

dataQueuePush = function(Matrix[Double] left, Matrix[Double] right, Matrix[Double] queue)  return (Matrix[Double] new_queue) {
    len = dataQueueLength(queue)
    if(len <= 0) {
        new_queue = cbind(left, right)
    } else {
        new_queue = cbind(queue, left, right)
    }
}

#-----------------------------------------------------------------------------------------------------------------------
# --------------------- Missuses matrix as a vectors for Doubles -------------------------------------------------------
dataVectorLength = function(Matrix[Double] vector)  return (Double len) {
    len = nrow(vector)
}

dataVectorGet = function(Matrix[Double] vector, Double index)  return (Double value) {
    value = as.scalar(vector[index, 1])
}
#-----------------------------------------------------------------------------------------------------------------------

calculateNodeDepth = function(Matrix[Double] node)  return(Double depth) {
    depth = log(as.scalar(node), 2) + 1
}

calculateChildNodes = function(Matrix[Double] node)  return(Matrix[Double] left, Matrix[Double] right) {
    left = node * 2.0
    right = node * 2.0 + 1.0
}

getTypeOfCol = function(Matrix[Double] R, Double col)  return(Double type) {    # 1..scalar,    2..categorical
    type = 1
}

calcImpurity = function() return (Double impurity, Matrix[Double] threshold) { #TODO IMPLEMENT
    impurity = as.scalar(rand(rows=1, cols=1, min=0, max=1, pdf="uniform", sparsity=1))
    threshold = rand(rows=1, cols=1, min=0, max=100, pdf="uniform", sparsity=1)
}

calcBestSplittingCriteria = function(
    Matrix[Double] X,
    Matrix[Double] Y,
    Matrix[Double] R,
    Matrix[Double] use_rows_vector,
    Matrix[Double] use_cols_vector)  return (Double impurity, Double used_col, Matrix[Double] threshold, Double type) {

    impurity = 1
    used_col = 1
    threshold = matrix(0, 1, 1)
    type = 1
    # -- user-defined function calls not supported for iterable predicates
    len = dataVectorLength(use_cols_vector)
    for (c_col in 1:len) {
        use_feature = dataVectorGet(use_cols_vector, c_col)
        if (use_feature != 0) {
            c_type = getTypeOfCol(R, c_col)
            [c_impurity, c_threshold] = calcImpurity()
            if(c_impurity < impurity) {
                impurity = c_impurity
                used_col = c_col
                threshold = c_threshold
                type = c_type
            }
        }
    }
}

m_decisionTree = function(
    Matrix[Double] X,
    Matrix[Double] Y,
    Matrix[Double] R,
    int bins = 20,
    int depth = 3,
    int num_leaf = 10,
    int num_samples = 3000,
    String impurity = "Gini",
    String fmt = "text"
)  return (Matrix[Double] M) {

    node_queue = matrix(1, rows=1, cols=1)             # Add first Node
    impurity_queue = matrix(1, rows=1, cols=1)
    use_cols_queue = matrix(1, rows=ncol(X), cols=1)   # Add fist bool Vector with all cols <=> (use all cols)
    use_rows_queue = matrix(1, rows=nrow(X), cols=1)   # Add fist bool Vector with all rows <=> (use all rows)
    # featureQueue = [inputData]
    # misclassifiedQueue = [-1]
    # depthQueue = [0]
    queue_length = 1

    while (queue_length > 0) {
        print("-------------------------------------------------------------------------------------------------------")
        [node_queue, node] = dataQueuePop(node_queue)
        print("Popped Node:        " + as.scalar(node))
        [use_rows_queue, use_rows_vector] = dataQueuePop(use_rows_queue)
        [use_cols_queue, use_cols_vector] = dataQueuePop(use_cols_queue)

        [impurity, used_col, threshold, type] = calcBestSplittingCriteria(X, Y, R, use_rows_vector, use_cols_vector)
        print("Current impurity:   " + impurity)
        print("Current column:     " + used_col)
        print("Current threshold:  " + as.scalar(threshold[1,1])) #TODO IMPLEMENT PRINT MATRIX
        print("Current type:       " + type)
        node_depth = calculateNodeDepth(node)
        if (node_depth < depth) {
            [left, right] = calculateChildNodes(node)
            node_queue = dataQueuePush(left, right, node_queue)

            #TODO SPLIT
            use_rows_queue = dataQueuePush(use_rows_vector, use_rows_vector, use_rows_queue)
            use_cols_queue = dataQueuePush(use_cols_vector, use_cols_vector, use_cols_queue)
        }
        queue_length = dataQueueLength(node_queue)# -- user-defined function calls not supported in relational expressions

        print("New QueueLen:       " + queue_length)
        print("-------------------------------------------------------------------------------------------------------")
        print(" ")
    }

    M = X
}
