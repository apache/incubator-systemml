#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# THIS SCRIPT IMPLEMENTS CLASSIFICATION TREES WITH BOTH SCALE AND CATEGORICAL FEATURES
#
# INPUT         PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE     DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             String   ---          Location to read feature matrix X; note that X needs to be both recoded and dummy coded
# Y 			String   ---		  Location to read label matrix Y; note that Y needs to be both recoded and dummy coded
# R   	  		String   " "	      Location to read the matrix R which for each feature in X contains the following information
#										- R[,1]: column ids
#										- R[,2]: start indices
#										- R[,3]: end indices
#									  If R is not provided by default all variables are assumed to be scale
# bins          Int 	 20			  Number of equiheight bins per scale feature to choose thresholds
# depth         Int 	 25			  Maximum depth of the learned tree
# num_leaf      Int      10           Number of samples when splitting stops and a leaf node is added
# num_samples   Int 	 3000		  Number of samples at which point we switch to in-memory subtree building
# impurity      String   "Gini"    	  Impurity measure: entropy or Gini (the default)
# M             String 	 ---	   	  Location to write matrix M containing the learned tree
# O     		String   " "          Location to write the training accuracy; by default is standard output
# S_map			String   " "		  Location to write the mappings from scale feature ids to global feature ids
# C_map			String   " "		  Location to write the mappings from categorical feature ids to global feature ids
# fmt     	    String   "text"       The output format of the model (matrix M), such as "text" or "csv"
# ---------------------------------------------------------------------------------------------
# OUTPUT:
# Matrix M where each column corresponds to a node in the learned tree and each row contains the following information:
#	 M[1,j]: id of node j (in a complete binary tree)
#	 M[2,j]: Offset (no. of columns) to left child of j if j is an internal node, otherwise 0
#	 M[3,j]: Feature index of the feature (scale feature id if the feature is scale or categorical feature id if the feature is categorical)
#			 that node j looks at if j is an internal node, otherwise 0
#	 M[4,j]: Type of the feature that node j looks at if j is an internal node: 1 for scale and 2 for categorical features,
#		     otherwise the label that leaf node j is supposed to predict
#	 M[5,j]: If j is an internal node: 1 if the feature chosen for j is scale, otherwise the size of the subset of values
#			 stored in rows 6,7,... if j is categorical
#			 If j is a leaf node: number of misclassified samples reaching at node j
#	 M[6:,j]: If j is an internal node: Threshold the example's feature value is compared to is stored at M[6,j] if the feature chosen for j is scale,
#			  otherwise if the feature chosen for j is categorical rows 6,7,... depict the value subset chosen for j
#	          If j is a leaf node 1 if j is impure and the number of samples at j > threshold, otherwise 0
# -------------------------------------------------------------------------------------------
# HOW TO INVOKE THIS SCRIPT - EXAMPLE:
# hadoop jar SystemDS.jar -f decision-tree.dml -nvargs X=INPUT_DIR/X Y=INPUT_DIR/Y R=INPUT_DIR/R M=OUTPUT_DIR/model
#     				                 				   bins=20 depth=25 num_leaf=10 num_samples=3000 impurity=Gini fmt=csv



# ----------------------------------------------------------------------------------------------------------------------
# Pseudo Code:
# All ignoring NULL Features/COLUMNS/ROWS
# calcImpurity(frame = Frame[], col, labels = Array[])
#       returns impurity: Scale and splittingCriteria: Scalar or List{FeatureClassIndices}
# calcBestSplittingCriteria(frame = Frame[], labels = Array[])
#       runs through all features in frame and calculates the impurity
#       returns column with the best (lowest) Impurity, and the splittingCriteria
# splitData(frame = Frame, splittingCriteria: SplittingCriteria)
#       returns FalseFrame and TrueFrame according to the Splitting Criteria (to keep the indices true fill unwanted Data with NULL)
# calcLeftNode(i: Int) = i * 2
#       returns it left NodeInBinTree (for Example: calcLeftNode(1) = 2, calcLeftNode(2) = 4, calcLeftNode(3) = 6)
# -------------------------
# inputData = read(X)
# labels = read(Y)
# inputLabels = (R == " ")? USE_SCALAR : read(R) USE_IT_TO_DETERMINE_IF_FEATURE_IS_SCALE_OR_LABELED
#
# featureQueue = [inputData]
# nodeQueue = [1]
# misclassifiedQueue = [-1]
# impurityQueue = [1.0]
# depthQueue = [0]
# decisionTreeOutputMatrix = [[]]
#
# while (featureQueue.isNotEmpty) {
#
#   currentDataFrame = featureQueue.getAndRemoveFirst()             --------| // Col and Row indices
#   currentNode = nodeQueue.getAndRemoveFirst()                             |
#   currentMisclassified = misclassifiedQueue.getAndRemoveFirst()           | - Could be one Class, if this was an Object Oriented Language
#   parentImpurity = impurityQueue.getAndRemoveFirst()                      |
#   parentDepth = depthQueue.getAndRemoveFirst()                    --------|
#
#   impurity, splittingCriteria = calcBestSplittingCriteria(currentDataFrame, labels)
#   if (impurity < parentImpurity AND parentDepth + 1 <= depth) {
#     #------------------------------------IS NODE!------------------------------------#
#     numMisclassifiedFalse, falseFrame, numMisclassifiedTrue, trueFrame = splitData(currentDataFrame, splittingCriteria)
#     featureQueue.putAll(falseFrame, trueFrame)
#     misclassifiedQueue.putAll(numMisclassifiedFalse, numMisclassifiedTrue)
#     nodeQueue.putAll(calcLeftNode(currentNode), calcLeftNode(currentNode) + 1)
#     impurityQueue.putAll(impurity, impurity)
#     depthQueue.putAll(parentDepth + 1, parentDepth + 1)
#
#     decisionTreeOutputMatrix.addColumn( [id = currentNode,
#                                          offset = length(featureQueue) - 1,
#                                          featureIndex = splittingCriteria.Column,
#                                          featureType = splittingCriteria.Type,                # 1 for scale and 2 for categorical features
#                                          numCategories = length(splittingCriteria.threshold), # look at description on the top of the page
#                                          splittingCriteria.threshold                          # threshold for scale else categories (element of operation)
#                                          ## append Categories if necessary ])
#   } else {
#     #------------------------------------IS LEAF!------------------------------------#
#     decisionTreeOutputMatrix.addColumn( [id = currentNode,
#                                          offset = 0,
#                                          featureIndex = 0
#                                          featureType = currentNode mod 2,                # (0 for false Branch, 1 for True Branch)
#                                          numCategories = currentMisclassified,
#                                          $1 if currentNode is impure and the number of samples at currentDataFrame > threshold, otherwise 0$])
#   }
# }

# --------------------- Missuses matrix as a queue for vectors ---------------------------------------------------------
dataQueuePop = function(Matrix[Double] queue)  return (Matrix[Double] new_queue, Matrix[Double] node) {
    #print("Rows: " + nrow(queue) + " Cols: " + ncol(queue))

    node = matrix(queue[,1], rows=nrow(queue), cols=1)

    if (ncol(queue) < 2) {
        new_queue = matrix(0,0,0)
    } else {
        new_queue = matrix(queue[1:1,2:ncol(queue)], rows=nrow(queue), cols=ncol(queue)-1)
    }
}

dataQueuePush = function(Matrix[Double] left, Matrix[Double] right, Matrix[Double] queue)  return (Matrix[Double] new_queue) {
    if(ncol(queue) <= 0) {
        new_queue = cbind(left, right)
    } else {
        new_queue = cbind(queue, left, right)       #Dafuq meme
    }
}

dataQueueLength = function(Matrix[Double] queue)  return (Double len) {
    len = ncol(queue)
}
#-----------------------------------------------------------------------------------------------------------------------

calculateChildNodes = function(Matrix[Double] node)  return(Matrix[Double] left, Matrix[Double] right) {
    left = node * 2.0
    right = node * 2.0 + 1.0
}

m_decisionTree = function(
    Matrix[Double] X,
    Matrix[Double] Y,
    Matrix[Double] R,
    int bins = 20,
    int depth = 25,
    int num_leaf = 10,
    int num_samples = 3000,
    String impurity = "Gini",
    String fmt = "text"
)  return (Matrix[Double] M) {

    node_queue = matrix(1, rows=1, cols=1)             # Add first Node
    queue_length = 1

    while (queue_length > 0) {
        [node_queue, node] = dataQueuePop(node_queue)

        if (as.scalar(node) <= 3) {
            [left, right] = calculateChildNodes(node)
            node_queue = dataQueuePush(left, right, node_queue)

        }
        queue_length = dataQueueLength(node_queue)
        # -- user-defined function calls not supported in relational expressions
        print("-------------------------------------------------------------------------------------------------------")
        print("Popped Node:  " + as.scalar(node))
        print("New QueueLen: " + queue_length)
        print("-------------------------------------------------------------------------------------------------------")
        print(" ")

    }

    M = X
}
