#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# Neural Collaborative Filtering
#

# Imports
source("nn/optim/adam.dml") as adam
source("nn/layers/relu.dml") as relu
source("nn/layers/sigmoid.dml") as sigmoid
source("nn/layers/affine.dml") as affine
source("nn/layers/l2_loss.dml") as l2_loss
source("nn/layers/l2_reg.dml") as l2_reg

train = function( matrix[double] users_train, 
                  matrix[double] items_train, 
                  matrix[double] targets_train, 
                  matrix[double] users_val, 
                  matrix[double] items_val, 
                  matrix[double] targets_val)
    return (List[unknown] biases, List[unknown] weights) {
  # /*
  #  * Train NCF model
  #  *
  #  * Inputs:
  #  *  TODO: elaborate
  #  *
  #  * Outputs:
  #  *  - biases: list of biases
  #  *  - weights: list o weights
  #  *
  #  * Network Architecture:
  #  * TODO: update
  #  * X --> [model] --> out --> l2_loss::backward(out, y) --> dout
  #  *
  #  */

  print("items_val ∈ " + nrow(items_val) + "×" + ncol(items_val));
  print("items_train ∈ " + nrow(items_train) + "×" + ncol(items_train));
  print("users_val ∈ " + nrow(users_val) + "×" + ncol(users_val));
  print("users_train ∈ " + nrow(users_train) + "×" + ncol(users_train));

  # sanity checks
  assert(nrow(items_train) == nrow(users_train));
  assert(nrow(users_train) == nrow(targets_train));
  assert(nrow(items_val) == nrow(users_val));
  assert(nrow(users_val) == nrow(targets_val));
  assert(ncol(items_val) == ncol(items_train));
  assert(ncol(users_val) == ncol(users_train));
  
  assert(ncol(targets_val) == ncol(targets_train));
  assert(ncol(targets_train) == 1);

  K_train = nrow(targets_train); # number of samples
  K_val = nrow(targets_val);

  N = ncol(items_train); # number items
  M = ncol(users_train); # number users

  E = 8; # embedding dimension

  print("NCF training starting with " 
          + K_train + " training samples, " 
          + K_val + " validation samples, " 
          + N + " items and "
          + M + " users...");

  # 1.initialize layers
  [UW1, Ub1] = affine::init(N, E); # user embedding
  [UW2, Ub2] = affine::init(E, 1); # final prediction

  # initialize bias and weight lists
  biases = list(Ub1, Ub2);
  weights = list(UW1, UW2);

  # 2.initialize adam optimizer
  ## Default values for some parameters
  lr      = 0.001;
  beta1   = 0.9;       # [0, 1)
  beta2   = 0.999;     # [0, 1)
  epsilon = 0.00000001;
  t       = 0;

  # user layer 1
  [mUW1, vUW1] = adam::init(UW1);
  [mUb1, vUb1] = adam::init(Ub1);
  # user layer 2
  [mUW2, vUW2] = adam::init(UW2);
  [mUb2, vUb2] = adam::init(Ub2);

  # regularization
  lambda = 0; #5e-04;

  # Optimize 
  batch_size = 10;
  epochs = 100000;
  N = K_train;
  iters = ceil(N / batch_size);

    for (e in 1:epochs) {
      for (i in 1:iters) {
        # Get the next batch
        beg = ((i-1) * batch_size) %% N + 1;
        end = min(N, beg + batch_size - 1);
        
        X_batch = items_train[beg:end,];
        y_batch = targets_train[beg:end,];

        # 3.Send inputs through layers
        [out, out_U1, out_U2] = predict(X_batch, biases, weights);

        # 4.compute gradients 
        dout = sigmoid::backward(out, y_batch); # (predictions, targets)

        # Compute loss & accuracy for training & validation data every 100 iterations.
        if (i %% 100 == 0) {
          # Compute training loss & accuracy
          [loss, accuracy] = eval(out, y_batch);
          # ignore regularization for now..
          # loss_reg_b1 = l2_reg::forward(b1, lambda);
          # loss_reg_W1  = l2_reg::forward(W1 , lambda);
          # loss = loss_data + loss_reg_b1 + loss_reg_W1;

          # Compute validation loss & accuracy
          probs_val = predict(items_val, biases, weights);
          [loss_val, accuracy_val] = eval(probs_val, targets_val);

          # Output results
          print("Epoch: " + e + ", Iter: " + i + ", Train Loss: " + loss + ", Train Accuracy: "
                 + accuracy + ", Val Loss: " + loss_val + ", Val Accuracy: " + accuracy_val)
        }

        # 5.Send the above result through affine::backward
        [dout_U2, dUW2, dUb2] = affine::backward(dout, out_U1, UW2, Ub2); # (gradient from upstream, input, weights, biases) 
        [dX, dUW1, dUb1] = affine::backward(dout_U2, X_batch, UW1, Ub1); 

        # 6.update timestep
        t = e * i - 1;

        # 7.Call adam::update for all parameters
        [Ub1, mUb1, vUb1] = adam::update(Ub1, dUb1, lr, beta1, beta2, epsilon, t, mUb1, vUb1);
        [UW1, mUW1, vUW1] = adam::update(UW1, dUW1, lr, beta1, beta2, epsilon, t, mUW1, vUW1);
        [Ub2, mUb2, vUb2] = adam::update(Ub2, dUb2, lr, beta1, beta2, epsilon, t, mUb2, vUb2);
        [UW2, mUW2, vUW2] = adam::update(UW2, dUW2, lr, beta1, beta2, epsilon, t, mUW2, vUW2);

        # 8. Update lists, TODO: find out if necessary
        biases = list(Ub1, Ub2);
        weights = list(UW1, UW2);
      }
    }
}

predict = function(matrix[double] X, List[unknown] biases, List[unknown] weights)
    return (matrix[double] out, matrix[double] out_U1, matrix[double] out_U2) {
  #
  # Computes the predictions for the given inputs.
  #
  # Inputs:
  #  - X : n examples with d features, of shape (n, d).
  #  - biases, weights : list of trained model parameters
  #
  # Outputs:
  #  - out : target vector, y.
  #
  
  # parse parameters
  Ub1 = as.matrix(biases[1]);
  Ub2 = as.matrix(biases[2]);

  UW1 = as.matrix(weights[1]);
  UW2 = as.matrix(weights[2]);

  # send inputs through layers
  out_U1 = affine::forward(X, UW1, Ub1);
  out_U2 = affine::forward(out_U1, UW2, Ub2);
  
  # TODO: add missing layers here
  
  out = out_U2; # final prediction
}

eval = function(matrix[double] probs, matrix[double] y)
    return (double loss, double accuracy) {
   /*
    * Computes loss and accuracy.
    */

    # compute the log loss
    loss = l2_loss::forward(probs, y);
    # TODO: does this also need to be adapted?

    # compute accuracy
    sqr_mean = mean( (probs - y)^2 )
    accuracy = (sqr_mean)^0.5

}
