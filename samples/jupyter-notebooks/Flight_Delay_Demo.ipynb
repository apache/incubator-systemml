{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction Demo Using SystemML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on datascientistworkbench.com's tutorial notebook for predicting flight delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SystemML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements to run this notebook:\n",
    "  1. Spark version 2.x \n",
    "  2. SystemML version 0.14 or above\n",
    "  2. Scala kernel (We have tried Toree kernel 0.2.0)\n",
    "  3. Add SystemML as a jars sub-option in spark_opts option when configure Toree Scala kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display SystemML version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.sysml.api.mlcontext.MLContext\n",
    "import org.apache.sysml.api.mlcontext.ScriptFactory.dml\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sparkSession = SparkSession.builder().master(\"local\").appName(\"Tutorial\").getOrCreate()\n",
    "val ml = new MLContext(sparkSession)\n",
    "\n",
    "print (\"Spark Version: \" + sc.version)\n",
    "print (\"\\nSystemML Version: \" + ml.version())\n",
    "print (\"\\nBuild Time: \" + ml.buildTime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the airline dataset from stat-computing.org if not already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "import java.net.URL\n",
    "import java.io.File\n",
    "val url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
    "val localFilePath = \"airline2007.csv.bz2\"\n",
    "if(!new java.io.File(localFilePath).exists) {\n",
    "    new URL(url) #> new File(localFilePath) !!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into DataFrame using Spark CSV package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sparkSession = SparkSession.builder.master(\"local\").appName(\"spark session example\").getOrCreate()\n",
    "val airline = sparkSession.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(localFilePath).na.replace(\"*\", Map(\"NA\" -> \"0.0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Which airports have the most delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline.registerTempTable(\"airline\")\n",
    "sparkSession.sql(\"\"\"SELECT Origin, count(*) conFlight, avg(DepDelay) delay\n",
    "                    FROM airline\n",
    "                    GROUP BY Origin\n",
    "                    ORDER BY delay DESC\"\"\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Logistic Regression\n",
    "\n",
    "Predict departure delays of greater than 15 of flights from JFK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import sparkSession.implicits._\n",
    "\n",
    "\n",
    "sparkSession.udf.register(\"checkDelay\", (depDelay:String) => try { if(depDelay.toDouble > 15) 1.0 else 2.0 } catch { case e:Exception => 1.0 })\n",
    "val tempSmallAirlineData = sparkSession.sql(\"SELECT *, checkDelay(DepDelay) label FROM airline WHERE Origin = 'JFK'\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "val popularDest = tempSmallAirlineData.select(\"Dest\").map(y => (y.get(0).toString, 1)).rdd.reduceByKey(_ + _).filter(_._2 > 1000).collect.toMap\n",
    "sparkSession.udf.register(\"onlyUsePopularDest\", (x:String) => popularDest.contains(x))\n",
    "tempSmallAirlineData.registerTempTable(\"tempAirline\")\n",
    "val smallAirlineData = sparkSession.sql(\"SELECT * FROM tempAirline WHERE onlyUsePopularDest(Dest)\")\n",
    "\n",
    "val datasets = smallAirlineData.randomSplit(Array(0.7, 0.3))\n",
    "val trainDataset = datasets(0).cache\n",
    "val testDataset = datasets(1).cache\n",
    "print (\"Training datasize = \" + trainDataset.count)\n",
    "print (\"\\nTest datasize = \" + testDataset.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the destination using one-hot encoding and include the columns Year, Month, DayofMonth, DayOfWeek, Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"Dest\").setOutputCol(\"DestIndex\").setHandleInvalid(\"skip\") // Only works on Spark 1.6 or later\n",
    "val encoder = new OneHotEncoder().setInputCol(\"DestIndex\").setOutputCol(\"DestVec\")\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Year\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"Distance\",\"DestVec\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model: Use SystemML's MLPipeline wrapper. \n",
    "\n",
    "This wrapper invokes MultiLogReg.dml (for training) and GLM-predict.dml (for prediction). These DML algorithms are available at https://github.com/apache/incubator-systemml/tree/master/scripts/algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.sysml.api.ml.LogisticRegression\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "val lr = new LogisticRegression(\"log\", sparkSession.sparkContext).setRegParam(1e-4).setTol(1e-2).setMaxInnerIter(0).setMaxOuterIter(100)\n",
    "val pipeline = new Pipeline().setStages(Array(indexer, encoder, assembler, lr))\n",
    "val model = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model \n",
    "\n",
    "Output RMS error on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val predictions = model.transform(testDataset.withColumnRenamed(\"label\", \"OriginalLabel\"))\n",
    "predictions.select(\"prediction\", \"OriginalLabel\").show\n",
    "sparkSession.udf.register(\"square\", (x:Double) => Math.pow(x, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.registerTempTable(\"predictions\")\n",
    "sparkSession.sql(\"SELECT sqrt(avg(square(OriginalLabel - prediction))) FROM predictions\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform k-fold cross-validation to tune the hyperparameters\n",
    "\n",
    "Perform cross-validation to tune the regularization parameter for Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "\n",
    "val crossval = new CrossValidator().setEstimator(pipeline).setEvaluator((new BinaryClassificationEvaluator).setRawPredictionCol(\"prediction\"))\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 1e-3, 1e-6)).build()\n",
    "crossval.setEstimatorParamMaps(paramGrid)\n",
    "crossval.setNumFolds(2) // Setting k = 2\n",
    "val cvmodel = crossval.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cvpredictions = cvmodel.transform(testDataset.withColumnRenamed(\"label\", \"OriginalLabel\"))\n",
    "cvpredictions.registerTempTable(\"cvpredictions\")\n",
    "sparkSession.sql(\"SELECT sqrt(avg(square(OriginalLabel - prediction))) FROM cvpredictions\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework ;)\n",
    "\n",
    "Read http://apache.github.io/incubator-systemml/algorithms-classification.html#multinomial-logistic-regression and perform cross validation on other hyperparameters: for example: icpt, tol, maxOuterIter, maxInnerIter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
