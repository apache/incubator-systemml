#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# This script performs dimensionality reduction using tSNE algorithm based on
# the paper: Visualizing Data using t-SNE, Maaten et. al.

X = read($X)

distance_matrix = function(matrix[double] X)
  return (matrix[double] out) {
    n = nrow(X)
    s = rowSums(X * X)
    out = - 2*X %*% t(X) + s + t(s)
  }

x2p = function(matrix[double] X, double tol, double perplexity)
  return(matrix[double] P) {
    INF = 1.0e20
    n = nrow(X)
    D = distance_matrix(X)

    P = matrix(0, rows=n, cols=n)
    beta = matrix(1, rows=n, cols=1)
    logU = log(perplexity)

    for (i in 1:n) {
      betamin = 0. # beta cannot be negative
      betamax = INF
      Hdiff = INF
      itr = 0
      if (i %% 100 == 0) {
        print(i)
      }

      Di = D[i,]
      while (abs(Hdiff) > tol & itr < 50) {
        Pi = exp(-Di * beta[i,1])
        P[1,i] = 0.
        sum_Pi = sum(Pi)

        H = log(sum_Pi) + beta[i,1] * sum(Di * Pi) / sum_Pi
        Pi = Pi/sum_Pi
        Hdiff = as.scalar(H - logU)

        if (Hdiff > 0.) {
          betamin = as.scalar(beta[i,1])
          if (betamax == INF) {
            beta[i,1] = beta[i,1] * 2.
          } else {
             beta[i,1] = (beta[i,1] + betamax) / 2.
          }
        } else {
          betamax = as.scalar(beta[i,1])
          if (betamin == 0.) {
            beta[i,1] = beta[i,1] / 2.
          } else {
            beta[i,1] = (beta[i,1] + betamin) / 2.
          }
        }
        itr = itr + 1
      }
      P[i,] = Pi
    }
    P = P + t(P)
    P = P / sum(P)
  }

tsne = function(matrix[double] X, int reduced_dims, int initial_dims, int perplexity)
  return(matrix[double] Y, matrix[double] C) {
    d = reduced_dims
    n = nrow(X)

    max_iter = 2000
    eta = 500
    P = x2p(X, 1.0e-5, 20.0)
    P = P*4
    Y = rand(rows=n, cols=d, pdf="normal")
    C = matrix(0, rows=max_iter, cols=1)
    ZERODIAG = (diag(matrix(-1, rows=n, cols=1)) + 1)

    for (itr in 1:max_iter) {
      D = distance_matrix(Y)
      Z = 1/(D + 1)
      Z = Z * ZERODIAG
      Q = Z/sum(Z)
      W = (P - Q)*Z
      sumW = rowSums(W)
      grad_C = Y * sumW - W %*% Y
      Y = Y - eta*grad_C
      Y = Y - colMeans(Y)

      if (itr%%50 == 0) {
        #C[itr,] = sum(P * log(pmax(P, 1e-12) / pmax(Q, 1e-12)))
        #print(as.scalar(C[itr,1]))
        print(itr)
      }
      if (itr == 100) {
        P = P/4
      }
    }
  }

[Y, C] = tsne(X, 2, 50, 20.0)

write(Y, $Y)
write(C, $C)
