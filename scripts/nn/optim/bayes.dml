#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/*
 * Bayesian Optimizer
 */
 
Sobol = function (matrix [double] X, int d, int n)
   return (matrix [double] quasi, matrix [double] seed) {
     n = nrow(X);
     
     /*
      * code for sobol script generator
      * 
      * Reference:
      * - Algorithm 659: Implementing Sobol’s Quasirandom Sequence Generator | Bratley & Fox
      *   - This is a modified version of above algorithm by Joe & Kuo 
      *      - doi>10.1145/641876.641879
      *  
      * Inputs:
      * - n: no. of points to generate
      * - d: spatial dimension (to make sure that the file contains enough data!)
      * - X: input file containing direction numbers
      *
      * Outputs:
      *  A matrix points, where
      *
      * POINTS = the j-th component of the i-th point # NxD matrix
      * 
      * with i indexed from 0 to N-1 and j indexed from 0 to D-1
      *
      */
      
      
      # ------------------------------------------ 
      #       N |   Binary  |   HI BIT |LOW BIT 
      # ------------------------------------------
      #       0           0      0       1
      #       1           1      1       2
      #       2          10      2       1
      #       3          11      2       3
      #       4         100      3       1
      #       5         101      3       2
      #       6         110      3       1
      #      10        1010      4       0
      #    1023  1111111111     10      11
      #    1024 10000000000     11       1
      #    1025 10000000001     11       2
      # ------------------------------------------
      
      r = matrix("0", d, n)
      
      # initialize (a part of) V
      v = matrix("1 1 1 1 1 1 1 1 1 1
                  1 1 1 1 1 1 1 1 1 1      
                  1 1 1 1 1 1 1 1 1 1
                  1 1 1 1 1 1 1 1 1 1 

                  1 3 1 3 1 3 3 1
                  3 1 3 1 3 1 1 3 1 3
                  1 3 1 3 3 1 3 1 3 1 
                  3 1 1 3 1 3 1 3 1 3
  
                  7 5 1 3 3 7 5 
                  5 7 7 1 3 3 7 5 1 1 
                  5 3 3 1 7 5 1 3 3 7
                  5 1 1 5 7 7 5 1 3 3

                  1 7 9 13 11 
                  1 3 7 9 5 13 13 11 3 15
                  5 3 15 7 9 13 9 1 11 7
                  5 15 1 15 11 5 3 1 7 9

                  9 3 27 
                  15 29 21 23 19 11 25 7 13 17
                  1 25 29 3 31 11 5 23 27 19 
                  21 5 1 17 13 7 15 9 31 9

                  37 33 7 5 11 39 63 
                  27 17 15 23 29 3 21 13 31 25
                  9 49 33 19 29 11 19 27 25

                  13
                  33 115 41 79 17 29 119 75 73 105
                  7 59 65 21 3 113 61 89 45 107

                  7 23 39")
            
      # set poly
      poly = matrix("1   3   7   11   13   19   25   37   59   47
                     61  55  41  67   97   91  109  103  115   131   
                     193 137 145 143 241   157 185  167  229   171
                     213 191 253 203 211  239  247 285   369   299")
      
      initialized = 1
      dim_max = 40
      dim_num_save = -1
      log_max = 30
      seed_save = -1

      initialize remaining rows of V
      2, dim_num+1

      # find the degree polynomial I from binary encoding

      J  = poly(1:dim_num+1,)
      d = 0


      # Expand this bit pattern to separate components of logical array INCLUD.

      J = poly(1:dim_num+1,)
      includ = matrix(0, d, 1)

      J2 = floor(J/2)
      includ = (J != 2 * J2)
      J = J2

      J = (, d+1, maxcol+1)
      newv = v(1:i, 1:j-m)

      newv = bitwise_xor(newv, v)
      v = newv

      # Multiply columns of V by appropriate power of 2

      v(0:dim_num, j) = v(0:dim_num, j-1) * 2

      # RECIPD is 1/(common denominator of the elements in V).

      recipd = 1.0 / (2*l)

      lastq = matrix(0, dim_num, 1)

      seed = floor(seed)

      if (seed < 0) {
        seed = 0
      } elseif( seed == 0) {
        l = 1
        lastq = matrix(0, dim_num, 1)
      } elseif(seed == seed_save + 1) {
        l = lo0(seed)
      } elseif(seed <= seed_save) {
        seed_save = 0
        l = 1
        lastq = matrix(0, dim_num, 1)

        L = bit_lo0(seed_temp(seed_save:seed))
        lastq = bitwise_xor(lastq, v)
  
        L = bit_lo0(seed)
      } elseif(seed_save + 1 < seed) {
        L = bit_lo0(seed_temp(seed_save:seed))
        lastq = bitwise_xor(lastq, v)

        L = bit_lo0(seed)
      }

      quasi = matrix(0, dim_num, 1)
      quasi = lastq * recipd
      lastq = bitwise_xor(lastq, v)

      seed_save = seed 
      seed = seed + 1
    
}

samples = function (double N, double burn, logdist, xx, widths, step_out) 
  return (matrix [double] out) {
  
  /* 
   * Generate H Gaussian process hyperparameter samples
   *
   * Reference:
   * - Slice sampling covariance hyperparameters of latent Gaussian models | Iain Murray, Ryan Prescott Adams.
   *   - https://arxiv.org/pdf/1006.0868 
   *
   * Inputs:
   * - N: No of samples to gather
   * - burn: (1x1) after burning period of this length
   * - logdist: (@fn) function logprobstar = logdist(xx)
   * - xx: (Dx1) initial state (or array with D elements)
   * - widths: (Dx1) set to true if widths may sometimes be far too small
   * - step_out: (bool)
   *
   *
   * Outputs:
   * - out: matrix with samples
   * 
   */
   
   # Initialization
   D = length(xx);
   samples = matrix(0, rows=D, cols=N);
   
   if (length(widths) == 1) {
     widths = repmat(widths, D, 1); #repmat?
   }
   
   log_Px = logdist(xx);
   
   # main loop
   for (i in 1:(N+burn)) {
     log_uprime = log(rand) + log_Px;
     
     # sweep through the axes
     for (d in 1:D) {
       x_l = xx;
       x_r = xx;
       xprime = xx;   
       
       # create a horizontal interval (x_l, x_r) enclosing xx
       r = rand(rows=1, cols=1, min=0, max=1, pdf=”uniform”, sparsity=0.2);
       x_l = xx(d) - r*widths(d);
       x_r = xx(d) + (1 -r)*widths(d);
       
       if (step_out) {
         while (logdist(x_l) > log_uprime) {
           x_l(d) = x_l(d) - widths(d);
         }
         
         while (logdist(x_r) > log_uprime) {
           x_r(d) = x_r(d) + widths(d);
         }
       }
       
       # Inner loop: Propose xprimes and shrink intervals until good one found.
       s = 0;
       while (1) {
         s = s +1;
         xprime = rand()*(x_r(d) - x_l(d)) + x_l(d);
         log_Px = logdist(xprime);
         
         if (log_Px > log_uprime) break;
         else {
           # shrink in
           if      (xprime(d) > xx(d)) x_r(d) = x_prime(d);
           else if (xprime(d) < xx(d)) x_l(d) = x_prime(d);
           else error;
         }
       }  
       xx(d) = xprime(d);
     }
     
     # Record samples
     if (i > burn) {
   #    samples(:, i -burn) = xx(:);# write to out matrix
     }
   }
 }


 next_val = function (matrix [double] X, matrix [double] y) 
            return (double x_next) {
   /*
    * Selects the next point to evaluate
    * 
    * Reference:
    *  - Practical Bayesian Optimization of Machine Learning Algorithms | Jasper, Hugo & Adams.
    *    - https://arxiv.org/pdf/1206.2944
    * 
    * Inputs:
    * - X: Observations
    * - y: Observations 
    *
    * Outputs:
    * - x_next: the next point to be evaluated
    *
    */
    
    X_cand = Sobol (M);
    
    for (h in 1:H) {
      /*
       * surrogate slice sample for theta(h)
       */
    }
    
    for (x_m in X_cand) {           # x_m in X_cand: evaluates to true if the x_m belongs to X_cand
      x_m = max_x ( for (h in 1:H) a(x_m; {x(m), y(m)}, theta(h)); )             
    }
    
    x_next = argmax_x ( for (h in 1:H) a(X_cand; {x(n), y(n)}, theta(h)); )
}
