#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/*
 * This layer implements the functionality of Caffe's SoftmaxWithLossLayer
 * Computes the multinomial logistic loss for a one-of-many classification task, 
 * passing real-valued predictions through a softmax to get a probability distribution over classes.
 * 
 *
 * Typical usage:
 * # Begin Forward Pass
 * prevOut = prevLayer::forward(...)
 * probs = softmax_loss::forward(prevOut)
 *
 * # Begin Backward Pass
 * prevDOut = softmax_loss::backward(prevOut, probs, yb)
 * ... = prevLayer::backward(prevDOut, ...)
 *
 * #  Compute training loss & accuracy
 * [loss, accuracy] = softmax_loss::compute_loss_accuracy(probs, y)
 */
 
 source("nn/layers/softmax.dml") as softmax
 source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
 
 forward = function(matrix[double] scores)
    return (matrix[double] probs) {
  /*
   * Computes the forward pass for SoftmaxWithLossLayer.  The input
   * has N examples, each with D values that are interpreted as
   * unnormalized, log-probabilities for each of D classes.  The softmax
   * function transforms these values to normalized probabilities across
   * the D classes, for every example.
   *
   * This can be interpreted as a generalization of the sigmoid
   * function to multiple classes.
   *
   *   `probs_ij = e^scores_ij / sum(e^scores_i)`
   *
   * Inputs:
   *  - scores: Inputs, of shape (N, D).
   *
   * Outputs:
   *  - probs: Outputs, of shape (N, D).
   */
   probs = softmax::forward(scores)
 }
 
 backward = function(matrix[double] scores, matrix[double] probs, matrix[double] y)
    return (matrix[double] dscores) {
  /*
   * Computes the backward pass for the SoftmaxWithLossLayer.  
   *
   * Note: The target y is one-hot encoded in this layer, whereas in Caffe
   * it is integer-valued with values [0, D-1].   
   *
   * Inputs:
   *  - scores: Inputs, of shape (N, D).
   *  - probs: Probabilities, of shape (N, D).
   *  - y: Targets, of shape (N, D).
   *
   * Outputs:
   *  - dscores: Gradient wrt `scores`, of shape (N, D).
   */
   dpred = cross_entropy_loss::backward(probs, y); 
   dscores = softmax::backward(dpred, scores);
 }
 
 compute_loss_accuracy = function(matrix[double] probs, matrix[double] y)
    return (double loss, double accuracy) {
  /*
   * Computes loss for the SoftmaxWithLossLayer.
   *
   * Note: The target y is one-hot encoded in this layer, whereas in Caffe
   * it is integer-valued with values [0, D-1].
   *
   * Inputs:
   *  - probs: Probabilities, of shape (N, D).
   *  - y: Targets, of shape (N, D).
   *
   * Outputs:
   *  - loss: Average loss.
   *  - accuracy: Average accuracy.
   */
   loss = cross_entropy_loss::forward(probs, y)
   predicted_y = rowIndexMax(probs)
   true_y = rowIndexMax(y)
   accuracy = mean(predicted_yb == true_yb)*100
 }
 