source("scripts/staging/fm-regression.dml")as fmRegression

source("nn/layers/affine.dml") as affine
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/dropout.dml") as dropout
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml")as softmax
source("nn/optim/adam.dml") as adam

train = function(matrix[double] X_train, matrix[double] y_train, matrix[double] X_val, matrix[double] y_val, int hidden_layer_neurons)
return (matrix[double] W1, matrix[double] b1, matrix[double] W2, matrix[double] b2, matrix[double] W3, matrix[double] b3)  {

# Generate input data
N = nrow(X_train) # num examples
D = ncol(X_train)# num features
t = 78 # num target cols

# Create network:
# batch -> affine1 -> relu1 -> dropout1 -> affine2 -> relu2 -> affine3 -> softmax
H1 = hidden_layer_neurons # number of neurons in 1st hidden layer
H2 = hidden_layer_neurons # number of neurons in 2nd hidden layer
p = 0.3  # dropout probability
[W1, b1] = affine::init(D, H1, -1)
[W2, b2] = affine::init(H1, H2, -1)
[W3, b3] = affine::init(H2, t, -1)

# Initialize Adams parameter
lr = 0.0001  # learning rate
mu = 0.5  # momentum
decay = 0.0001  # learning rate decay constant for weight decay

beta1   = 0.9;       # [0, 1)
beta2   = 0.999;     # [0, 1)
epsilon = 0.00000001;
#t       = 0; # t von oben?

# Adams optimizer
[mW1, vW1] = adam::init(W1);[mb1, vb1] = adam::init(b1)
[mW2, vW2] = adam::init(W2);[mb2, vb2] = adam::init(b2)
[mW3, vW3] = adam::init(W3);[mb3, vb3] = adam::init(b3)

# Optimize
print("Starting optimization")
batch_size = 256 #?
epochs = 100
iters = ceil(N / batch_size)
for (e in 1:epochs) {
  for(i in 1:iters){
    # Get next batch
    X_batch = X_train[i:i+batch_size-1,]
    y_batch = y_train[i:i+batch_size-1,]

    # Compute forward pass
    ## layer 1:
    out1 = affine::forward(X_batch, W1, b1)
    outr1 = relu::forward(out1)
    [outd1, maskd1] = dropout::forward(outr1, p, -1)
    ## layer 2:
    out2 = affine::forward(outd1, W2, b2)
    outr2 = relu::forward(out2)
    ## layer 3:
    out3 = affine::forward(outr2, W3, b3)
    probs = softmax::forward(out3)

    if (i==1) {
      # Compute loss
      loss = cross_entropy_loss::forward(probs, y_batch)
      accuracy = mean(rowIndexMax(probs) == rowIndexMax(y_batch))

      # Compute validation loss & accuracy
      probs_val = predict(X_val, W1, b1, W2, b2, W3, b3)
      loss_val = cross_entropy_loss::forward(probs_val, y_val)
      accuracy_val = mean(rowIndexMax(probs_val) == rowIndexMax(y_val))

      # Output results
      print("Epoch: " + e + ", Iter: " + i + ", Train Loss: " + loss + ", Train Accuracy: "
      + accuracy + ", Val Loss: " + loss_val + ", Val Accuracy: " + accuracy_val)
    }


    # Compute backward pass
    ## loss:
    dprobs = cross_entropy_loss::backward(probs, y_batch)
    ## layer 3:
    dout3 = softmax::backward(dprobs, out3)
    [doutr2, dW3, db3] = affine::backward(dout3, outr2, W3, b3)
    ## layer 2:
    dout2 = relu::backward(doutr2, out2)
    [doutd1, dW2, db2] = affine::backward(dout2, outd1, W2, b2)
    ## layer 1:
    doutr1 = dropout::backward(doutd1, outr1, p, maskd1)
    dout1 = relu::backward(doutr1, out1)
    [dX_batch, dW1, db1] = affine::backward(dout1, X_batch, W1, b1)

    # Optimize with Adam
    [W1, mW1, vW1] = adam::update(W1, dW1, lr, beta1, beta2, epsilon, t, mW1, vW1)
    [b1, mb1, vb1] = adam::update(b1, db1, lr, beta1, beta2, epsilon, t, mb1, vb1)
    [W2, mW2, vW2] = adam::update(W2, dW2, lr, beta1, beta2, epsilon, t, mW2, vW2)
    [b2, mb2, vb2] = adam::update(b2, db2, lr, beta1, beta2, epsilon, t, mb2, vb2)
    [W3, mW3, vW3] = adam::update(W3, dW3, lr, beta1, beta2, epsilon, t, mW3, vW3)
    [b3, mb3, vb3] = adam::update(b3, db3, lr, beta1, beta2, epsilon, t, mb3, vb3)
    }

  # Anneal momentum towards 0.999
  mu = mu + (0.999 - mu)/(1+epochs-e)
  # Decay learning rate
  lr = lr * decay
  }
}

predict = function(matrix[double] test_val,
matrix[double] W1, matrix[double] b1,
matrix[double] W2, matrix[double] b2,
matrix[double] W3, matrix[double] b3)
return (matrix[double] probs) {

N = nrow(test_val)
K = ncol(W3) # num features

# Network:
# conv1 -> relu1 -> pool1 -> conv2 -> relu2 -> pool2 -> affine3 -> relu3 -> affine4 -> softmax

# Compute predictions over mini-batches

probs = matrix(0, rows=N, cols=K)

batch_size = 256 #?
iters = ceil(N / batch_size)
for(i in 1:iters) {
# Get next batch
#X_batch = X_train[i:i+batch_size-1,] #in train
beg = ((i-1) * batch_size) %% N + 1
end = min(N, beg + batch_size - 1)
X_batch = test_val[beg:end,]

# Compute forward pass
## layer 1:
out1= affine::forward(X_batch, W1, b1)
outr1 = relu::forward(out1)
#[outd1, maskd1] = dropout::forward(outr1, p, -1) #dropout needed for predict?
## layer 2:
out2 = affine::forward(outr1, W2, b2)
outr2 = relu::forward(out2)
## layer 3:
out3 = affine::forward(outr2, W3, b3)
probs_batch = softmax::forward(out3)

# Store predictions
probs[beg:end,] = probs_batch
}
}

eval = function(matrix[double] probs, matrix[double] Y)
return (double loss, double accuracy) {
/*
* Evaluates a convolutional net using the "LeNet" architecture.
*
* The probs matrix contains the class probability predictions
* of K classes over N examples.  Thetargets, Y, have K classes,
* and are one-hot encoded.
*
* Inputs:
*  -probs: Class probabilities, of shape(N, K).
*  - Y: Target matrix, of shape (N, K).
*
* Outputs:
*  - loss: Scalar loss, of shape (1).
*  - accuracy: Scalar accuracy, of shape(1).
*/
# Compute loss & accuracy
#print("Predicted probs: " +toString(probs))
loss = cross_entropy_loss::forward(probs, Y)
correct_pred = rowIndexMax(probs) == rowIndexMax(Y)
write(correct_pred, "../sherlock-project/data/data/short/correct_predicted_lines")
#print("correct prediction: " + toString(correct_pred))
accuracy = mean(correct_pred)
}