#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

#
# Neural Collaborative Filtering
#

# Imports
source("nn/optim/adam.dml") as adam
source("nn/layers/relu.dml") as relu
source("nn/layers/affine.dml") as affine
source("nn/layers/l2_loss.dml") as l2_loss
source("nn/layers/l2_reg.dml") as l2_reg

train = function( matrix[double] users_train, 
                  matrix[double] items_train, 
                  matrix[double] targets_train, 
                  matrix[double] users_val, 
                  matrix[double] items_val, 
                  matrix[double] targets_val)
    return (/* matrix[double] w0, matrix[double] W, matrix[double] V */) {
  # TODO: return weights and biases and everything later
  # /*
  #  * Train NCF model
  #  *
  #  * Inputs:
  #  *  TODO: elaborate
  #  *
  #  * Outputs:
  #  *  - w0, W, V : model parameters.
  #  *
  #  * Network Architecture:
  #  * TODO: update
  #  * X --> [model] --> out --> l2_loss::backward(out, y) --> dout
  #  *
  #  */

  # sanity checks
  assert(nrow(items_train) == nrow(users_train));
  assert(nrow(users_train) == nrow(targets_train));
  assert(nrow(items_val) == nrow(users_val));
  assert(nrow(users_val) == nrow(targets_val));
  
  # FIXME: transform s.t. this works when checking with validation later! 
  # assert(ncol(items_val) == ncol(items_train));
  # assert(ncol(users_val) == ncol(users_train));
  
  assert(ncol(targets_val) == ncol(targets_train));
  assert(ncol(targets_train) == 1);

  K_train = nrow(targets_train); # number of samples
  K_val = nrow(targets_val);

  N = ncol(items_train); # number items
  M = ncol(users_train); # number users

  print("NCF training starting with " 
          + K_train + " training samples, " 
          + K_val + " validation samples, " 
          + N + " items and "
          + M + " users...");

  # 1.initialize layers
  [W1, b1] = affine::init(N, 1);

  # 2.initialize adam optimizer
  ## Default values for some parameters
  lr      = 0.001;
  beta1   = 0.9;       # [0, 1)
  beta2   = 0.999;     # [0, 1)
  epsilon = 0.00000001;
  t       = 0;

  [mW1, vW1] = adam::init(W1);
  [mb1, vb1] = adam::init(b1);

  # regularization
  lambda = 0; #5e-04;

  # Optimize 
  batch_size = 10;
  epochs = 1000;
  N = K_train;
  iters = ceil(N / batch_size);

    for (e in 1:epochs) {
      for (i in 1:iters) {
        # Get the next batch
        beg = ((i-1) * batch_size) %% N + 1;
        end = min(N, beg + batch_size - 1);
        
        X_batch = items_train[beg:end,];
        y_batch = targets_train[beg:end,];

        # 3.Send inputs through affine::forward
        out = affine::forward(X_batch, W1, b1);

        # 4.compute gradients from a loss l2_loss::backward
        dout = l2_loss::backward(out, y_batch);# (predictions, targets)

        # Compute loss & accuracy for training & validation data every 100 iterations.
        if (i %% 100 == 0) {
          # Compute training loss & accuracy
          [loss_data, accuracy] = eval(out, y_batch);
          loss_reg_b1 = l2_reg::forward(b1, lambda);
          loss_reg_W1  = l2_reg::forward(W1 , lambda);
          loss = loss_data + loss_reg_b1 + loss_reg_W1;

          # Compute validation loss & accuracy
          probs_val = predict(items_val, b1, W1)
          [loss_val, accuracy_val] = eval(probs_val, targets_val);

          # Output results
          print("Epoch: " + e + ", Iter: " + i + ", Train Loss: " + loss + ", Train Accuracy: "
                 + accuracy + ", Val Loss: " + loss_val + ", Val Accuracy: " + accuracy_val)
        }

        # 5.Send the above result through affine::backward
        [dX, dW1, db1] = affine::backward(dout, X_batch, W1, b1);

        # 6.update timestep
        t = e * i - 1;

        # 7.Call adam::update for all parameters
        [b1, mb1, vb1] = adam::update(b1, db1, lr, beta1, beta2, epsilon, t, mb1, vb1);
        [W1, mW1, vW1] = adam::update(W1, dW1, lr, beta1, beta2, epsilon, t, mW1, vW1);
      }
    }

  # TODO: return proper values in the end
  w0 = matrix(0, rows=10, cols=10)
  W = matrix(0, rows=10, cols=10)
  V = matrix(0, rows=10, cols=10)
}

predict = function(matrix[double] X, matrix[double] b1, matrix[double] W1)
    return (matrix[double] out) {
  /*
   * Computes the predictions for the given inputs.
   *
   * Inputs:
   *  - X : n examples with d features, of shape (n, d).
   *  - w0, W, V : trained model parameters.
   *
   * Outputs:
   *  - out : target vector, y.
   */

    # 1.Send inputs through affine::forward
    out = affine::forward(X, W1, b1);

}

eval = function(matrix[double] probs, matrix[double] y)
    return (double loss, double accuracy) {
   /*
    * Computes loss and accuracy.
    */

    # compute the log loss
    loss = l2_loss::forward(probs, y);

    # compute accuracy
    sqr_mean = mean( (probs - y)^2 )
    accuracy = (sqr_mean)^0.5

}
