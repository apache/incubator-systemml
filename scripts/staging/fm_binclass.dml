#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------


/**
 * Factorization Machines for binary classification.
 */

# Imports
source("staging/fm.dml") as fm
source("nn/layers/sigmoid.dml") as sigmoid
source("nn/layers/log_loss.dml") as log_loss
source("nn/optim/adam.dml") as adam

train = function(matrix[double] X)
    return (matrix[double] Y, matrix[double] X_val, matrix[double] Y_val) {
   /*
    * Trains a FM model.
    */
    
    # 1.initialize fm core
    [w0, W, V] = fm::init(d, k);
    
    # 2.initialize adam optimizer
    [mw0, vw0] = adam::init(w0);
    [mW, vW] = adam::init(W);
    [mV, vV] = adam::init(V);
    
    # 3.Send inputs through fm::forward
    y = fm::forward(X, w0, W, V);  
    
    # 4.Send the above result through sigmoid::forward
    sfy = sigmoid::forward(y);
    
    # 5.Send the above result through log_loss::forward
    lfy = log_loss::forward(sfy);
    
    # 6.Send the result of sigmoid::forward and the correct labels y to log_loss::backward
    lby = log_loss::backward(sfy, y);
    
    # 7.Send the above result through sigmoid::backward
    sby = sigmoid::backward(lby);
    
    # 8.Send the above result through fm::backward
    fb = fm::backward(sby);
        
    # 9.Call adam::update for all parameters
    # Default values for some parameters
    lr      = 0.001; 
    beta1   = 0.9;       # [0, 1)
    beta2   = 0.999;     # [0, 1)
    epsilon = 0.00000001; 
    t       = 0;

    adam::update(w0, g_w0, lr, beta1, beta2, epsilon, t, matrix[double] m, matrix[double] v);
    adam::update(W, g_W, lr, beta1, beta2, epsilon, t, matrix[double] m, matrix[double] v );
    adam::update(V, g_V, lr, beta1, beta2, epsilon, t, matrix[double] m, matrix[double] v );
    
}

predict = function(matrix[double] X)
    return (matrix[double] out) {
   /*
    * computes the predictions for the given inputs.
    */
    
    # 1.initialize fm core
    [w0, W, V] = fm::init(d, k);
    
    # 2.Send inputs through fm::forward
    y = fm::forward(X, w0, W, V);
    
    # 3.Send the above result through sigmoid::forward
    sfy = sigmoid::forward(y);
    
    # 4.Send the above result through log_loss::forward
    lfy = log_loss::forward(sfy); 
  
}

eval = function(matrix[double] probs, matrix[double] y)
    return (double loss) {
   /*
    * Computes loss and accuracy.
    */
    
    # just compute loss with log_loss::forward(probs, y)
    loss = log_loss::forward(probs, y);
}
