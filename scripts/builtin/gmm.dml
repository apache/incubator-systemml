#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# ------------------------------------------
# Guassian Mixture Model
# ------------------------------------------

# INPUT PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME            TYPE    DEFAULT     MEANING
# ---------------------------------------------------------------------------------------------
# X               Double   ---       Matrix X  
# n_components      Integer  3         Number of n_components in the Gaussian mixture model
# iterations      Integer  10        number of iterations
# model           String   "V"       Univariate mixture    "VVV": unequal variance (full)
#                                                          "EEE": equal variance (tied)
#                                    Multivariate mixture  "VVI": spherical, unequal volume (Diag) 
#                                                          "VII": spherical, equal volume (Spherical)
# eps            Double    0.000001  value for convergence 
# ---------------------------------------------------------------------------------------------


#Output(s)
# ---------------------------------------------------------------------------------------------
# NAME            TYPE    DEFAULT     MEANING
# ---------------------------------------------------------------------------------------------
# weight          Double   ---       A matrix whose [i,k]th entry is the probability that observation i in the test data belongs to the kth class




m_gmm = function(Matrix[Double] X, Integer n_components = 1, String model = "VVV", Integer iter = 100, String init_params = "kmeans", Double eps = 0.000001, Boolean verbose = FALSE )
return (Matrix[Double] weights)
{
  # sanity checks
  if(model != "VVV" & model != "EEE" & model != "VVI" & model != "VII")
    stop("model not supported, should be in VVV, EEE, VVI, VII");
  
  [nk, mu, sigma] = initialize_param(X, n_components, init_params, model)
  
  prec_chol = compute_precision_cholesky(sigma, method)
  # Determine the initial GMM paramters from Kmeans
  # μ, σ, weights and phi
  n_sample = nrow(X)
  
  
  phi = matrix(1/n_components, 1, n_components)
  weights = matrix(1/n_components, nrow(X), n_components)
  
  [C, Y] = kmeans(X,  n_components, 10, 10, eps, FALSE, 25)
  mu = C
  sigma = list()
  
  for(i in 1:n_components)
    sigma = append(sigma, covMatrix(X, matrix(1, nrow(X), 1)) ) #diag(matrix(1, ncol(X), 1))

  loglikPre = mean(log(rowSums(weights*phi)))
  loglikDiff = 1
  i = 1
  bic = matrix(0, iter, 1)
  a = n_components * ncol(X) * (ncol(X) + 1) / 2
  b = ncol(X) * n_components
  df = a + b + n_components -1
  # df = (ncol(X)*ncol(X) - ncol(X))/2 + (2*ncol(X) + 1)
  # df = n_components * df -1
  print("df "+df)
  while(i <= iter & loglikDiff >= eps ){
    print("iteration ----------------------------------"+i)
     # 
    # Estimation Step
    [weights, loglikNew] = predict_prob(X, n_components, mu, sigma, phi)
    phi = colMeans(weights)
    print("phi "+toString(phi))
    # Maximization Step - Update parameter
  
    for(j in 1:n_components)
    {
      wk = weights[, j]
      sum_wk = sum(wk)
      mu[j, ] = colSums(X*wk)/sum_wk
      sigma[j] = covMatrix(X, (wk/sum_wk)) # A covariance matrix instead of single covariance value
      print("mu m "+toString(mu[j, ]))
      print("sigma m "+toString(as.matrix(sigma[j])))
    }
    loglikDiff = abs(loglikNew - loglikPre)
    loglikPre = loglikNew
    bic[i,1] = (-2 * loglikNew * nrow(X) + df * log(nrow(X)))
    i = i+1
  }
  
  # predict 
  predict = rowIndexMax(weights)
  if(verbose) {
    print("logliklihood diff "+loglikDiff)
    print("logliklihood "+loglikNew)
    print("BIC for each iteration "+toString(bic))
    print("predictions \n"+toString(predict))
    print("weights \n"+toString(weights))
  }

}

covMatrix = function(Matrix[Double] X, Matrix[Double] w)
return (Matrix[Double] covM) 
{ 
  covM = matrix(0, ncol(X), ncol(X))
  if(ncol(X) < 2 )
    covM[1,1] = var(X)

  wmu = colSums(X*w)/sum(w)
  Xc = X - wmu
  # R = (t(Xc) %*% Xc)/(nrow(X)-1)/ (t(colSds(X)) %*% colSds(X)); #cov(X)
  # print("this is wsigma "+toString(R));
  sig = 1/(sum(w)) * (t(Xc * w) %*% Xc) 
    
  covM = sig  
  # print("this is wsigma "+toString(sig))
  # for(i in 1:ncol(X))
    # for(j in 1:ncol(X))
      # covM[i, j] = cov(X[,i], X[,j], w)
}

predict_prob = function(Matrix[Double] X, Integer n_components, Matrix[Double] mu, List[Unknown] sigma, Matrix[Double] phi)
  return (Matrix[Double] weights, Double loglik)
{
  likelihood = matrix(0, nrow(X), n_components )
  for(i in 1:n_components)
  {

    likelihood[, i] = mvpdf(X, mu[i,], as.matrix(sigma[i]))
  }
  numer = likelihood * phi
  denom = rowSums(numer)
  loglik = mean(log(denom)) 
 
  weights = numer/denom

}

mvpdf = function(Matrix[Double] X, Matrix[Double] mu, Matrix[Double] sigma) 
return (Matrix[Double] pdf)
{
  pdf = matrix(0, nrow(X), 1)
  # [eva, evec] = eigen(sigma)
  # det = colProds(eva)
  # det = as.scalar(det)
  # print("this is eval "+det)
  det = determinant(sigma, nrow(sigma))
    print("det "+det)
  if (det == 0) {
    stop("Determinant is equal to 0.")
  }
  d = ncol(X)
  for(i in 1:nrow(X))
  {
    x_m = X[i, ] - mu 
    p1 = 1/sqrt((2*pi)^d*det) 
    p2 = -0.5 * x_m %*% inverse(sigma) %*% t(x_m)
    p3 = p1 * exp(p2)
    pdf[i,1] += as.scalar(p1 * exp(p2))
  }
  # print("pdf "+toString(pdf))
}

determinant = function(Matrix[Double] X, Double n)
  return (Double det)
{      
 
  # temporary array for storing row  
  # int []temp = new int[n + 1];  
  num1=0; num2=0; detr = 1;index=0;total = 1;
  temp = matrix(0, n+1, 1)        
  # loop for traversing the diagonal elements  
  for(i in 1:n)  
  {  
    index = i; # initialize the index  
              
    # finding the index which has non zero value  
    while(as.scalar(X[index, i]) == 0 & index < n) 
    {  
      index =  index + 1;      
                  
    }  
    if(index == n)# if there is non zero element  
    {  
    # the determinat of matrix as zero  
                  
    }  
    if(index != i)  
    {  
      # loop for swaping the diagonal element row and index row  
      for(j in 1:n)  
      {  
        swap(X, index, j, i, j); 
        print("after swap "+toString(X))
      }  
      # determinant sign changes when we shift rows  
      # go through determinant properties  
        detr = as.integer(detr* (-1)^(index-i));  
    }  
          
    # storing the values of diagonal row elements  
    for(j in 1:n)  
    {  
      temp[j,1] = X[i,j];  
                  
    }  
      
    # traversing every row below the diagonal element 
    j = i+1
    while(j<=n)  
    {  
      num1 = as.scalar(temp[i,1]);  # value of diagonal element  
      num2 = as.scalar(X[j, i]); # value of next row element  
                  
      # traversing every column of row  
      # and multiplying to every row  
      for(k in 1:n)  
      {  
                  
        # multiplying to make the diagonal  
        # element and next row element equal  
        X[j, k] = (num1 * X[j, k]) - (num2 * temp[k, 1]);  
                      
      }  
      total = total * num1; # Det(kA)=kDet(A);
      j = j+1
    }  
              
  }  
      
    # mulitplying the diagonal elements to get determinant  
  for(i in 1:n)  
  {  
    detr = detr * as.scalar(X[i, i]);  
              
  }  
  det = (detr/total); # Det(kA)/k=Det(A);  
} 
 
  
swap =  function(Matrix[Double] X, Integer i1, Integer j1, Integer i2, Integer j2) 
  # return (Matrix[Double] X1)
{ 
  print("before swap "+toString(X))
  temp = as.scalar(X[i1, j1])
    # int temp = arr[i1][j1];
  X[i1, j1] = as.scalar(X[i2, j2]); 
  X[i1, j1] = as.scalar(X[i2, j2])
  X[i2, j2] = temp; 
  # X1 = X
}
  
initialize_param = function(Matrix[Double] X, Integer n_components, String init_params, String model)
return (Matrix[Double] weight, Matrix[Double] mean, List[Unknown] sigma)
{
  reg_covar = 1e-6
  l = list()
  print("com "+n_components)
  n =  nrow(X)
  d = ncol(X)
  resp = matrix(0,n, n_components)
  if(init_params == "kmeans")
  {

    [C, Y] = kmeans(X, n_components, 10, 10, 0.0000001, FALSE, 25)
    print("y \n"+toString(Y))
    resp = resp + t(seq(1,n_components))
    print("resp" +toString(resp))
    # resp = outer(Y, resp, "==")
    resp = resp == Y
    
  }
  else if(init_params == "random")
  {
    resp = Rand(rows = n, cols=n_components)   
    resp = resp/rowSums(resp)
  }
  else stop("invalid paramter value, expected kmeans or random found "+init_params)
  print("resp matrix \n"+toString(resp))
  nk = colSums(resp)
  print("nk "+toString(nk))
  mu = (t(resp) %*% X) / t(nk)
  sigma = list()
  if(model == "VVV")
  { 
    for(k in 1:n_components)
    {
      diff = X - mu[k,]
      cov = (t(diff * resp[, k]) %*% diff) / nk[,k]
      cov = cov + diag(matrix(reg_covar, ncol(cov), 1))
      sigma = append(sigma, cov)
    }      
  }
  else if(model == "EEE")
  { 
    avgX2 = t(X) %*% X
    avgMean = t(mu * nk) %*% mu
    cov = avgX2 - avgMean
    cov /= sum(nk)
    cov = cov + diag(matrix(reg_covar, ncol(cov), 1))
    sigma = append(sigma, cov)
  }
  else if(model ==  "VVI")
  {
    avgX2 = (t(resp) %*% (X*X)) / t(nk)
    avgMean = mu ^ 2
    avgMean2 = mu * (t(resp) %*% X) / t(nk)
    cov = avgX2 - 2 * avgMean + avgMean2 + reg_covar
    sigma = append(sigma, cov)
  }
  else if (model == "VII")
  {
    avgX2 = (t(resp) %*% (X*X)) / t(nk)
    avgMean = mu ^ 2
    avgMean2 = mu * (t(resp) %*% X) / t(nk)
    cov = avgX2 - 2 * avgMean + avgMean2 + reg_covar
    sigma = append(sigma, colMeans(cov))
  }
  weight = nk
  mean = mu
}

compute_precision_cholesky = function(List[Unknown] sigma, String model)
return (List[Unknown] precision_chol)
{
  precision_chol = list()
  d = ncol(as.matrix(sigma[1]))
  if(model == "VVV")
  { 
    comp = length(sigma)
    for(k in 1:length(sigma))
    {
      cov_chol = cholesky(as.matrix(sigma[k]))
      pre_chol = t(solve(cov_chol, diag(matrix(1, d, 1))))
      precision_chol = append(precision_chol, pre_chol)
    }      
  }
  else if(model == "EEE")
  { 
    cov_chol = cholesky(as.matrix(sigma[1]))
    pre_chol = t(solve(cov_chol, diag(matrix(1, d, 1))))
    precision_chol = append(precision_chol, pre_chol)
  }
  else{
    mat = as.matrix(sigma[1])
    if(sum(mat <= 0) > 0)
      stop("Fitting the mixture model failed because some components have 
        ill-defined empirical covariance (for instance caused by singleton 
        or collapsed samples). Try to decrease the number of components, 
        or increase reg_covar.")
    else
    {
      precision_chol = append(precision_chol, 1.0/sqrt(mat))
    }   
  }
}

e_step = function()
return(Double norm, Matrix[Double] log_resp){
  weighted_log_prob = self._estimate_weighted_log_prob(X)
  log_prob_norm = logsumexp(weighted_log_prob, axis=1)
  with np.errstate(under='ignore'):
  # ignore underflow
  log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
    norm = mean(log_prob_norm)
}

estimate_weighted_log_prob = function(Matrix[Double] X)
return (Matrix[Double] weight_log_pro)
{
  weight_log_pro = estimate_log_prob(X) + estimate_log_weights()
}
  
estimate_log_weights = function(Matrix[Double] w)
return (Matrix[Double] log_weight)
{
  log_weight = log(w)
}

estimate_log_prob = function(Matrix[Double] X)
return (Matrix[Double] log_prob)
{
  est_weight_log_pro = estimate_log_gaussian_prob(
            X, self.means_, self.precisions_cholesky_, self.covariance_type)
}

compute_log_det_cholesky = function(Matrix[Double] mat_chol, String model, Integer d)
return(Matrix[Double] log_det_prec_chol)
{
  log_det_prec_chol = list()
  if(model == "VVV")
  { 
    comp = length(mat_chol)
     
    for(k in 1:n_components)
    {
      mat_chol = mat_chol * diag(matrix(1, d, 1))
      log_det = colSums(log(as.matrix(mat_chol[k])))   # have to take the log of diag elements only
      log_det_prec_chol = append(log_det_prec_chol, log_det)
    }      
  }
  else if(model == "EEE")
  { 
    avgX2 = t(X) %*% X
    avgMean = t(mu * nk) %*% mu
    cov = avgX2 - avgMean
    cov /= sum(nk)
    cov = cov + diag(matrix(reg_covar, ncol(cov), 1))
    sigma = append(sigma, cov)
  }
  else if(model ==  "VVI")
  {
    avgX2 = (t(resp) %*% (X*X)) / t(nk)
    avgMean = mu ^ 2
    avgMean2 = mu * (t(resp) %*% X) / t(nk)
    cov = avgX2 - 2 * avgMean + avgMean2 + reg_covar
    sigma = append(sigma, cov)
  }
  else if (model == "VII")
  {
    avgX2 = (t(resp) %*% (X*X)) / t(nk)
    avgMean = mu ^ 2
    avgMean2 = mu * (t(resp) %*% X) / t(nk)
    cov = avgX2 - 2 * avgMean + avgMean2 + reg_covar
    sigma = append(sigma, colMeans(cov))
  }
}