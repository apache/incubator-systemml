# INPUT         PARAMETERS:
# ---------------------------------------------------------------------------------------------
# NAME          TYPE     DEFAULT      MEANING
# ---------------------------------------------------------------------------------------------
# X             String   ---          Location to read feature matrix X; note that X needs to be both recoded and dummy coded
# Y 			String   ---		  Location to read label matrix Y; note that Y needs to be both recoded and dummy coded
# R   	  		String   " "	      Location to read the matrix R(nx1) which for each feature in X contains the following information
#										- R[,2]: 1 (scalar feature)
#										- R[,1]: 2 (categorical feature)
# ---------------------------------------------------------------------------------------------

# ---------------------------------------------------------------------------------------------
# OUTPUT:
# Matrix M where each column corresponds to a node in the learned tree and each row contains the following information:
#  M[1,j]: id of node j (in a complete binary tree)
#  M[2,j]: tree id to which node j belongs
#  M[3,j]: Offset (no. of columns) to left child of j if j is an internal node, otherwise 0
#  M[4,j]: Feature index of the feature (scale feature id if the feature is scale or categorical feature id if the feature is categorical)
#    that node j looks at if j is an internal node, otherwise 0
#  M[5,j]: Type of the feature that node j looks at if j is an internal node. if leaf = 0, if scalar = 1, if categorical = 2
#  M[6:,j]: If j is an internal node: Threshold the example's feature value is compared to is stored at M[6,j] if the feature chosen for j is scale,
#     otherwise if the feature chosen for j is categorical rows 6,7,... depict the value subset chosen for j
#     If j is a leaf node 1 if j is impure and the number of samples at j > threshold, otherwise 0
# -------------------------------------------------------------------------------------------

m_xgboost = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] R) return (Matrix[Double] M) {
  print("[DEBUG] inside xgboost")

  # test if input correct
  assert(nrow(X) == nrow(y))
  assert(ncol(y) == 1)
  assert(nrow(R) == 1)

  M = matrix(0,rows=6,cols=0)
  init_prediction = median(y)

   tree_id = 1
   # todo: iterator with while/for for each tree
  new_created_tree_M = buildOneTree(tree_id)

  node_queue = matrix(1, rows=1, cols=1)     # Add first Node
  curr_rows_queue = matrix(1, rows=nrow(X), cols=1)
  node_queue_len = 1
  max_depth = 6
  depth = 0.0
  level = 0

  # adjust this here if we create more trees to create new prediction values
  while (node_queue_len > 0) {
    has_child = FALSE
    [node_queue, node] = dataQueuePop(node_queue)
    [curr_rows_queue, curr_rows_vector] = dataQueuePop(curr_rows_queue)

    printMatrix(node_queue, "Node Queue")
    printMatrix(node, "Node - bei welcher node sind wir gerade?")
    printMatrix(curr_rows_queue, "curr_rows_queue")
    printMatrix(curr_rows_vector, "curr_rows_vector - welche rows sollen wir jz in betracht ziehen?")

    level = log(as.scalar(node), 2) + 1
    available_rows = calcAvailableRows(curr_rows_vector) # check if leaf
    print("avaliable rows " + toString(available_rows))

    [curr_X, curr_y, curr_X_full] = updateMatrices(X, y, curr_rows_vector)

    printMatrix(curr_X, "curr_X - alle rows die jz fÃ¼r mich relevant sind")
    printMatrix(curr_y, "curr_y - y zu rows die relevant sind")
    printMatrix(curr_X_full, "curr_X_full - gleich wie X, aber die nicht relevanten rows sind 0")

    best_feature_index = 0.00
    if(available_rows > 1 & max_depth > level) # leaf check or max depth check
    {
      best_feature_index = findBestFeature(X=curr_X, y=curr_y, verbose=FALSE)
      print("best freatures index " + best_feature_index)
      # maybe for forest change here prediction?

      type = getTypeOfFeature(R, best_feature_index)

      if(type == 1.0) # SCALAR
      {
        residual_matrix = calculateResiduals(curr_y, median(curr_y))
        # printMatrix(residual_matrix, "the calculated residuals")

        similarity_score = calculateSimilarityScore(residual_matrix)

        [best_split_threshold, best_gain] = findBestSplit(curr_X[,best_feature_index], similarity_score)
        has_child = best_gain > 0 # if the gain is < 0, the split is worse than the current node
      }
      else # CATEGORICAL
      {
        # TODO: always true?
        assert(type == 2.0)
        has_child = TRUE
      }
    }

    if (has_child)
    {
      [left, right] = calculateChildNodeIDs(node)
      node_queue = dataQueuePush(left, right, node_queue)

      if (type == 1.0) # SCALAR
      {
          # print("[DEBUG] SCALAR:")
          [left_row_vec, right_row_vec] = splitMatrixByValueLoop(curr_X_full[,best_feature_index], X[,best_feature_index], best_split_threshold)
          curr_rows_queue = dataQueuePush(left_row_vec, right_row_vec, curr_rows_queue)
          offset = ncol(node_queue) - 1
          M = addOutputRow(M, node, tree_id, R, offset, best_feature_index, best_split_threshold)

      }
      else # CATEGORICAL
      {
          assert(type == 2.0)
          # print("[DEBUG] CATEGORY:")
          [left_row_vec, right_row_vec] = splitMatrixByCategory(curr_X_full[,best_feature_index], X[,best_feature_index])
          printMatrix(left_row_vec, "left row vector after categorical split")
          printMatrix(right_row_vec, "right row vector after categorical split")
          curr_rows_queue = dataQueuePush(left_row_vec, right_row_vec, curr_rows_queue)
          offset = ncol(node_queue) - 1
          M = addOutputRow(M, node, tree_id, R, offset, best_feature_index, 0.0)

      }
    }
    else # has no child
    {
      M = addOutputRow(M, node, tree_id, R, 0.0, best_feature_index, 0.0)
    }
    node_queue_len = ncol(node_queue)
    printMMatrix(M)
  }
  print("[DEBUG] we successfully created one tree - lets make another one")

  # todo pruning
  # todo create next tree depending on previous
  print("[DEBUG] Return xgboost")
}


#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    tree_id: the id of the new creating tree
# OUTPUT:   current_tree_M: The M matrix of this new created tree
buildOneTree = function(Double tree_id) return (Matrix[Double] current_tree_M) {

    current_tree_M = matrix(0, rows=6, cols=1)
}



#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    node: a 1xn matrix (node containing values)
# OUTPUT:   left: the new left node id
# OUTPUT:   right: the new right node id
calculateChildNodeIDs = function(Matrix[Double] node)  return(Matrix[Double] left, Matrix[Double] right) {
  left = node * 2.0
  right = node * 2.0 + 1.0
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    M: the nxn output matrix
# INPUT:    node: the current node
# R   	  	The matrix R which for each feature in X contains the following information
#					- R[,2]: 1 (scalar feature)
#				    - R[,1]: 2 (categorical feature)
# INPUT:    offset: offset to the left child
# INPUT:    used_col: which feature got used
# INPUT:
# INPUT:    threshold: the value at the node we separated it
# OUTPUT:   new_M: nxn output Matrix
addOutputRow = function(
  Matrix[Double] M,
  Matrix[Double] node,
  Double tree_id,
  Matrix[Double] R,
  Double offset,
  Double used_col,
  Double threshold
) return (Matrix[Double] new_M) {

      print("best feature into M " + used_col)

      current_node = matrix(0, rows=6, cols=1)
      current_node[1,1] = node[1,1] # node id
      current_node[2,1] = tree_id
      current_node[3,1] = offset # offset to left child
      current_node[4,1] = used_col # features used
      # change here to different stuff if it is classification

      if(used_col == 0.0) { # is a single value or max depth reached => leaf
         current_node[5,1] = 0 #  0 if leaf node
         assert(threshold < 1.00)
      }
      else if (as.scalar(R[1, used_col]) == 2.0) {
        current_node[5, 1] = 2.0 # categorical
      }
      else {
        current_node[5,1] = 1 # 1 if it is scalar
      }
      current_node[6,1] = threshold # split value ( 0 if leaf or categorical)
      if (ncol(M) == 0 & nrow(M) == 0) { # first node
        new_M = current_node
      } else {
        new_M = cbind(M, current_node)
      }
}


#-----------------------------------------------------------------------------------------------------------------------
# INPUT:   R: Location to read the matrix R(nx1) which for each feature in X contains the following information
#					- R[,2]: 1 (scalar feature)
#			    	- R[,1]: 2 (categorical feature)
# INPUT:    col: the col that is desired to know the type
# OUTPUT:   [1.0,scalar] or  [2.0,categorical]
#-----------------------------------------------------------------------------------------------------------------------
getTypeOfFeature = function(Matrix[Double] R, Double col)  return(Double type) {
  type = as.scalar(R[1, col])
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    X: a nxn matrix (X Matrix from input)
# INPUT:    y: a 1xn matrix (y Matrix from input)
# INPUT:    vector: a 1xn matrix (contains 1 for relevant and 0 for non-relevant value)
# OUTPUT:   new_X: the new X containing only the relevant rows
# OUTPUT:   new_y: the new y containing only the relevant rows
# OUTPUT:   new_X_full: the new X containing the relevant rows + 0 for non_relevant
updateMatrices = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] vector)
  return(Matrix[Double] new_X, Matrix[Double] new_y, Matrix[Double] new_X_full) {
  # change here to have the full matrices but not to cut it from 4x4 to 2x2 rather 4x4 but with 0 in the unused fields and then we gucci
  new_X = matrix(0, rows = 0, cols = 0)
  new_y = matrix(0, rows = 0, cols = 0)
  zero_vec = matrix(0, rows=nrow(vector), cols=ncol(X))
  new_X_full = matrix(0, rows = 0, cols=0)
  for(i in 1:nrow(vector)) {
    if(as.scalar(vector[i,]) > 0.0)
    {
      if (ncol(new_X) == 0 & ncol(new_X_full) == 0) {
        new_X = X[i,]
        new_y = y[i,]
        new_X_full = X[i,]
      } else if (ncol(new_X) == 0 & ncol(new_X_full) != 0) {
        new_X = X[i,]
        new_y = y[i,]
        new_X_full = rbind(new_X_full, X[i,])
      } else {
        new_X = rbind(new_X, X[i,])
        new_y = rbind(new_y, y[i,])
        new_X_full = rbind(new_X_full, X[i,])
      }
    } else { # is 0
      if (ncol(new_X_full) == 0) {
       new_X_full = zero_vec[i,]
      } else {
       new_X_full = rbind(new_X_full, zero_vec[i,])
      }
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    vector: a 1xn matrix (current datapoints which are interesting)
# OUTPUT:   available_elements: nr. of available datapoints
calcAvailableRows = function(Matrix[Double] vector) return(Double available_elements){
  assert(ncol(vector) == 1)
  len = nrow(vector)
  available_elements = 0.0
  for (index in 1:len) {
    element = as.scalar(vector[index,])
    if(element > 0.0) {
      available_elements = available_elements + 1.0
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    queue: a nxn matrix (queue containing values)
# OUTPUT:   new_queue: the new queue after the first vector gets popped
# OUTPUT:   node_vec: the popped vector (1xn) from the queue
dataQueuePop = function(Matrix[Double] queue)  return (Matrix[Double] new_queue, Matrix[Double] node_vec) {
  node_vec = matrix(queue[,1], rows=1, cols=nrow(queue)) # reshape to force the creation of a new object
  node_vec = matrix(node_vec, rows=nrow(queue), cols=1)  # reshape to force the creation of a new object
  len = ncol(queue)
  if (len < 2) {
    new_queue = matrix(0,0,0)
  } else {
    new_queue = matrix(queue[,2:ncol(queue)], rows=nrow(queue), cols=ncol(queue)-1)
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    left: a 1xn matrix
# INPUT:    right: a 1xn matrix
# INPUT:    queue: a nxn matrix (queue containing values)
# OUTPUT:   new_queue: the new queue nxn matrix
dataQueuePush = function(Matrix[Double] left, Matrix[Double] right, Matrix[Double] queue)  return (Matrix[Double] new_queue) {
  len = ncol(queue)
  if(len <= 0) {
    new_queue = cbind(left, right)
  } else {
    new_queue = cbind(queue, left, right)
  }
}


findBestFeature = function(Matrix[Double] X, Matrix[Double] y, boolean verbose = FALSE) return (Integer lowest_residuals_index) {

   printDebug("Finding Best feature")

   lowest_residuals = 0
   lowest_residuals_index = 1

   for(i in 1: ncol(X)) {
        current_feature = X[,i]
        weights = lm(X=current_feature, y=y, verbose=verbose)
        y_residual = y - current_feature %*% weights
        res = sum(y_residual ^ 2)                # sum of least square calculation
#        res = abs(sum (y_residual) / nrow (X)); # average calculation

        if(verbose) {
            printDebug("currentFeature col " + toString(i))
            printDebug("weights of lm " + toString(weights))
            printDebug("feature residuals: " + res)
            print("--------------------------------------")
        }

        if(i == 1 | res < lowest_residuals) {
            lowest_residuals = res
            lowest_residuals_index = i
        }
   }
   printDebug("best feature for current node is: " + toString(lowest_residuals_index) + " with residuals: " + toString(lowest_residuals))

}


#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    one_featureX: a 1xn matrix (one feature with all values)
# OUTPUT:   best_split: the best split (highest gain indicates best splitting of datasets)
findBestSplit = function(Matrix[Double] one_featureX, Double sim_score_parent)
  return (Double best_split, Double best_gain)
{
    assert(ncol(one_featureX) == 1)
    best_split = 0
    best_gain = 0
    best_sim_score_left = 0
    best_sim_score_right = 0
    # printDebug("inside find best split")

    # order entries by feature value
    ordered_X = orderFeature(one_featureX)
    # printMatrix(ordered_X, "ordered X")

    # iterate over all averages between 2 points
    for(i in 1: (nrow(ordered_X)-1)) {

        current_split = average(ordered_X[i,], ordered_X[i+1,])
        # printDebug("first average = " + toString(current_split))
        [left, right] = splitMatrixByValue(one_featureX, current_split)
        # printMatrix(left, "left matrix")
        # printMatrix(right, "right matrix")

        # calculate similarity score for that split and calculate gain
        sim_score_left = calculateSimilarityScore(left)
        sim_score_right = calculateSimilarityScore(right)
        # printDebug("sim score left " + toString(sim_score_left) + "   sim score right " + toString(sim_score_right))
        current_gain = sim_score_left + sim_score_right - sim_score_parent
        # printDebug("current gain " + toString(current_gain))

        if(current_gain > best_gain)
        {
            best_gain = current_gain
            best_split = current_split
            best_sim_score_left = sim_score_left
            best_sim_score_right = sim_score_right
            # printDebug("best gain changed to  " + toString(best_gain) + " at value " + toString(best_split))
        }
    }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    X: a 1xn matrix (one feature with all values)
#           value: the border to separate the values
#           loop: boolean to check if used in loop and returns then other values
# OUTPUT:   left: a 1xn matrix with all values smaller than "value"
#           right: a 1xn matrix with all values larger than "value"
splitMatrixByValue = function(Matrix[Double] X, Double value) return (Matrix[Double] left, Matrix[Double] right) {
  assert(ncol(X) == 1)
  # printDebug("in splitMatrixByValue")
  # printDebug("split at " + value)
  left = matrix(0, rows=0, cols=1)
  right = matrix(0, rows=0, cols=1)

  for(i in 1:nrow(X)) {
    # printDebug("current to look " + as.scalar(X[i,]))
    if(as.scalar(X[i,]) < value) {
      left = rbind(left,X[i,])
    }
    else {
      right = rbind(right, X[i,])
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    curr_X: a 1xn matrix (one feature with current values)
#           X:     a 1xn matrix the feature of the X-Matrix with all values
#           value: the border to separate the values
# OUTPUT:   left: a 1xn matrix with 1.0 if the value is smaller or 0.0 if it is bigger
#           right: a 1xn matrix with 1.0 if the value is bigger or 0.0 if it is smaller
splitMatrixByValueLoop = function(Matrix[Double] curr_X, Matrix[Double] X, Double value) return (Matrix[Double] left, Matrix[Double] right) {
  left = matrix(0, rows=0, cols=1) # TODO why rows 0??
  right = matrix(0, rows=0, cols=1)
  one_vec = matrix(1, rows=1, cols=1)
  zero_vec = matrix(0, rows=1, cols=1)

  for(i in 1:nrow(X)) {
    if (as.scalar(X[i,]) == as.scalar(curr_X[i,])) { # if same row in curr_X and X
      if (as.scalar(X[i,]) < value) {
        left = rbind(left, one_vec[1,]) # add 1 to left and 0 to right vector
        right = rbind(right, zero_vec[1,])
      }
      else {
        right = rbind(right, one_vec[1,]) # add 1 to right and 0 to left vector
        left = rbind(left, zero_vec[1,])
      }
    } else {
      left = rbind(left, zero_vec[1,]) # add 0 to both vectors
      right = rbind(right, zero_vec[1,])
    }
  }
}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    curr_X: a 1xn matrix (one feature with current values)
#           X:     a 1xn matrix the feature of the X-Matrix with all values
# OUTPUT:   left: a 1xn matrix with 1.0 for if the value is true or 0.0 if it is false
#           right: a 1xn matrix with 1.0 for if the value is false or 0.0 if it is true
splitMatrixByCategory = function(Matrix[Double] curr_X, Matrix[Double] X) return (Matrix[Double] left, Matrix[Double] right) {
  left = matrix(0, rows=0, cols=1)
  right = matrix(0, rows=0, cols=1)

  for(i in 1:nrow(X)) {
      if (as.scalar(X[i,]) == 1) {
        left = rbind(left, X[i,])
        right = rbind(right, matrix(0, rows=1, cols=1))
      }
      else {
        assert(as.scalar(X[i,]) == 0) #
        left = rbind(left, X[i,])
        right = rbind(right, matrix(1, rows=1, cols=1))
      }
    }

}

#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    row_vector: a 1xn matrix (one feature with all residuals)
# OUTPUT:   similarity_score: the similarity score of the residuals
calculateSimilarityScore = function (matrix[Double] row_vector) return (Double similarity_score) {
  lambda = 0.0; # TODO
  similarity_score = (sum(row_vector)^2) / (nrow(row_vector) + lambda);
  #printDebug("Calculated similarity score: " + similarity_score);
}


#-----------------------------------------------------------------------------------------------------------------------
# INPUT:    row_vector: a 1xn matrix (one feature with all rows)
#           prediction: the current prediction value
# OUTPUT:   the residuals from the "prediction" as 1xn matrix
calculateResiduals = function (matrix[Double] row_vector, Double prediction) return (matrix[Double] residuals_row) {
   assert(ncol(row_vector) == 1)
   residuals_row = row_vector - prediction;
}

orderFeature = function (matrix[double] row_vector) return (matrix[double] row_vector_ordered) {
    row_vector_ordered = order(target=row_vector, by=1, decreasing=FALSE, index.return=FALSE);
}

average = function(Matrix[Double] x, Matrix[Double] y) return (Double avg) {
    assert(ncol(x) == 1 & nrow(x) == 1)
    assert(ncol(y) == 1 & nrow(y) == 1)
    avg = (as.scalar(x) + as.scalar(y)) / 2
}

printDebug = function(string variable) return() {
    print("[DEBUG] " + variable)
}

printMatrix = function(Matrix[Double] m, string name) return() {
    print("--------------------------------------")
    print(":: [MATRIX] " + name + "  (col:" + ncol(m) + ",row:" + nrow(m) + ")")

    if(nrow(m) != 0 & ncol(m) != 0)
    {
        col_string = ":::::::"
        for(col in 1:ncol(m)) {
            if(col < 10)
                col_string = col_string + ":::(" + toString(col) + ")::::"
            else
                col_string = col_string + ":::(" + toString(col) + "):::"
        }
        print(col_string)
        for(row in 1:nrow(m)) {

            print_string = ":(" + row + ")     "
            for(col in 1:ncol(m)) {
                curr_value = m[row,col]
                if(as.scalar(curr_value) < 10)
                    print_string = print_string + toString(curr_value, linesep="     ", sep=",")
                else if(as.scalar(curr_value) < 100)
                    print_string = print_string + toString(curr_value, linesep="    ", sep=",")
                else if(as.scalar(curr_value) < 1000)
                    print_string = print_string + toString(curr_value, linesep="   ", sep=",")
                else if(as.scalar(curr_value) < 10000)
                    print_string = print_string + toString(curr_value, linesep="  ", sep=",")
                else
                    print_string = print_string + toString(curr_value, linesep=" ", sep=",")
            }
            print(print_string)
        }
    }
    print("--------------------------------------")
}


printMMatrix = function(Matrix[Double] m) return() {
    print("--------------------------------------")
    print(":: [MATRIX] OUTPUT M MATRIX  (col:" + ncol(m) + ",row:" + nrow(m) + ")")

    if(nrow(m) != 0 & ncol(m) != 0)
    {
        col_string = "::::::::::::::::::::::::::::::::"
        for(col in 1:ncol(m)) {
            if(col < 10)
                col_string = col_string + ":::(" + toString(col) + ")::::"
            else
                col_string = col_string + ":::(" + toString(col) + "):::"
        }
        print(col_string)
        for(row in 1:nrow(m)) {

            if(row == 1)
                print_string = "                 node id :(" + row + ")     "
            if(row == 2)
                print_string = "                 tree id :(" + row + ")     "
            if(row == 3)
                print_string = "    offset to left child :(" + row + ")     "
            if(row == 4)
                print_string = "  used feature col index :(" + row + ")     "
            if(row == 5)
                print_string = "if leaf=0/scalar=1/cat=2 :(" + row + ")     "
            if(row == 6)
                print_string = "       scalar=splitvalue :(" + row + ")     "


            for(col in 1:ncol(m)) {
                curr_value = m[row,col]
                if(as.scalar(curr_value) < 10)
                    print_string = print_string + toString(curr_value, linesep="     ", sep=",")
                else if(as.scalar(curr_value) < 100)
                    print_string = print_string + toString(curr_value, linesep="    ", sep=",")
                else if(as.scalar(curr_value) < 1000)
                    print_string = print_string + toString(curr_value, linesep="   ", sep=",")
                else if(as.scalar(curr_value) < 10000)
                    print_string = print_string + toString(curr_value, linesep="  ", sep=",")
                else
                    print_string = print_string + toString(curr_value, linesep=" ", sep=",")
            }
            print(print_string)
        }
    }
    print("--------------------------------------")
}