#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/**
 * Factorization Machines for Regression.
 */

# Imports
source("nn/optim/adam.dml") as adam
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/fm.dml") as fm
source("nn/layers/l2_loss.dml") as l2_loss
source("nn/layers/l2_reg.dml") as l2_reg
source("nn/layers/log_loss.dml") as log_loss
source("nn/layers/sigmoid.dml") as sigmoid

n = 50 # num examples
d = 500 # num features
k = 2 # factorization dimensionality
X = rand(rows=n, cols=d);
y = rand(rows=n, cols=1);
X_val = rand(rows=n, cols=d)
y_val = rand(rows=n, cols=1)

train = function(matrix[double] X, matrix[double] y, matrix[double] X_val, matrix[double] y_val)
    return (matrix[double] w0, matrix[double] W, matrix[double] V) {
  /*
   * Trains the FM model.
   *
   * Inputs:
   *  - X     : n examples with d features, of shape (n, d)
   *  - y     : Target matrix, of shape (n, 1)
   *  - X_val : Input validation data matrix, of shape (n, d)
   *  - y_val : Target validation matrix, of shape (n, 1)
   * Outputs:
   *  - w0, W, V : updated model parameters.
   *
   * Network:
   *
   * X --> [model] --> out ----------------------> l2_loss::backward(sfy,out) --> dy
   *                    \                           /
   *                     sigmoid::forward(out) --> sfy
   *
   *
   */


    # 1.initialize fm core
    [w0, W, V] = fm::init(d, k);

    # 2.initialize adam optimizer
    ## Default values for some parameters
    lr      = 0.001;
    beta1   = 0.9;       # [0, 1)
    beta2   = 0.999;     # [0, 1)
    epsilon = 0.00000001;
    t       = 0;

    [mw0, vw0] = adam::init(w0);
    [mW, vW]   = adam::init(W);
    [mV, vV]   = adam::init(V);

    # regularization
    lambd = 5e-04

    # Optimize
    print("Starting optimization")
    batch_size = 64
    epochs = 10
    iters = ceil(N / batch_size)

    for (e in 1:epochs) {
      for (i in 1:iters) {
        # Get the next batch
        beg = ((i-1) * batch_size) %% N + 1
        end = min(N, beg + batch_size - 1)
        X_batch = X[beg:end,]
        y_batch = y[beg:end,]

        # 3.Send inputs through fm::forward
        out = fm::forward(X_batch, w0, W, V);

        # 4.Send the above result through log_loss::forward
        #sfy = sigmoid::forward(out);

        # 5.compute gradients from a loss l2_loss::backward
        #dy = l2_loss::backward(sfy, out);
        dout = l2_loss::backward(y_batch, out)# (predictions, targets)

        # Compute loss & accuracy for training & validation data every 100 iterations.
        if (i %% 100 == 0) {
          # Compute training loss & accuracy
          #loss_data = cross_entropy_loss::forward(sfy, y_batch)
          loss_data  = l2_loss::forward(out, y_batch)
          loss_reg_w0 = l2_reg::forward(w0, lambda)
          loss_reg_W  = l2_reg::forward(W , lambda)
          loss_reg_V  = l2_reg::forward(V , lambda)
          loss = loss_data + loss_reg_w0 + loss_reg_W + loss_reg_V
          accuracy = mean(rowIndexMax(sfy) == rowIndexMax(y_batch))


          # Compute validation loss & accuracy
          probs_val = predict(X_val, w0, W, V)
          #loss_val  = cross_entropy_loss::forward(probs_val, y_val)
          loss_val = l2_loss::forward(probs_val, y_val)
          accuracy_val = mean(rowIndexMax(probs_val) == rowIndexMax(y_val))

          # Output results
          print("Epoch: " + e + ", Iter: " + i + ", Train Loss: " + loss + ", Train Accuracy: "
                 + accuracy + ", Val Loss: " + loss_val + ", Val Accuracy: " + accuracy_val)
        }

        # 6.Send the above result through fm::backward
        #[dw0, dW, dV] = fm::backward(dy, X, w0, W, V);
        [dw0, dW, dV] = fm::backward(dout, X_batch, w0, W, V);

        # 6.Call adam::update for all parameters
        w0 = adam::update(w0, dw0, lr, beta1, beta2, epsilon, t, mw0, vw0);
        W  = adam::update(W, dW, lr, beta1, beta2, epsilon, t, mW, vW );
        V  = adam::update(V, dV, lr, beta1, beta2, epsilon, t, mV, vV );

      }
    }
}

predict = function(matrix[double] X, matrix[double] w0, matrix[double] W, matrix[double] V)
    return (matrix[double] out) {
  /*
   * Computes the predictions for the given inputs.
   *
   * Inputs:
   *  - X : n examples with d features, of shape (n, d).
   *  - w0, W, V : trained model parameters.
   *
   * Outputs:
   *  - out : target vector, y.
   */

    # 1.initialize fm core
    #[w0, W, V] = fm::init(d, k);

    # 2.Send inputs through fm::forward
    out = fm::forward(X, w0, W, V);

    # 3.Send the above result through log_loss::forward
    # loss = log_loss::forward(out);

}

eval = function(matrix[double] probs, matrix[double] y)
    return (double loss) {
   /*
    * Computes loss and accuracy.
    */

    # just compute loss with log_loss::forward(probs, y)
    #loss = log_loss::forward(probs, y)
    loss = l2_loss::forward(probs, y);

    # compute accuracy
    
}
